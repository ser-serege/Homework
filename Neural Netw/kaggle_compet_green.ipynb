{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "from itertools import count\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "from keras.layers import Dense, BatchNormalization, Flatten, Conv2D, MaxPooling2D, Dropout, Input\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG_W = 28\n",
    "IMG_H = 28\n",
    "BATCH_SIZE = 500\n",
    "TEST_SIZE = 750\n",
    "NUM_EPOCHS = 200\n",
    "NUM_CLASSES = 12\n",
    "NUM_CHANNELS = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_images(path, extract_class=True):\n",
    "    for index, file in enumerate(glob(path), 1):\n",
    "        cls = file.split('\\\\')[1] if extract_class else None\n",
    "        bgr = cv2.imread(file)\n",
    "        bgr = cv2.resize(bgr, dsize=(IMG_H, IMG_W), interpolation=cv2.INTER_CUBIC)\n",
    "        hsv = cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)\n",
    "        mask = cv2.inRange(\n",
    "            hsv,\n",
    "            (24, 50, 0),\n",
    "            (55, 255, 140)\n",
    "        )\n",
    "        img = np.concatenate([bgr, hsv, mask[..., np.newaxis]], axis=-1)\n",
    "        print(f'{index:04d}: {file}')\n",
    "        yield img, cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      
      "0794: test\\ffc6f8527.png\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "X_train, Y_train = zip(*load_images(path='train\\*\\*.png', extract_class=True))\n",
    "X_train = np.array(X_train)\n",
    "Y_train = to_categorical(le.fit_transform(Y_train), num_classes=NUM_CLASSES)\n",
    "\n",
    "X_test, _ = zip(*load_images(path='test\\*.png', extract_class=False))\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4750, 28, 28, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=42)\n",
    "for train_index, valid_index in splitter.split(X_train, Y_train):\n",
    "    X_train, X_valid = X_train[train_index], X_train[valid_index]\n",
    "    Y_train, Y_valid = Y_train[train_index], Y_train[valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Programs\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image.py:1183: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (4000, 28, 28, 7) (7 channels).\n",
      "  ' channels).')\n"
     ]
    }
   ],
   "source": [
    "aug_generator = ImageDataGenerator(\n",
    "    width_shift_range = 3 / IMG_W,\n",
    "    height_shift_range = 3 / IMG_H,\n",
    "    rotation_range = 90,\n",
    "#     shear_range = 5,\n",
    "    zoom_range = (0.8, 1.2),\n",
    "    fill_mode = 'constant',\n",
    "    cval = 0.0,\n",
    "    horizontal_flip = True,\n",
    "    vertical_flip = True,\n",
    ")\n",
    "\n",
    "aug_generator.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfB7vo8bJkD4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x196d5524748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calc_learning_rate(epoch):\n",
    "    return 0.005 * math.pow(0.75, (1 + epoch) / 10)\n",
    "\n",
    "epoch = np.linspace(0, 100, 500)\n",
    "lrate = list(map(calc_learning_rate, epoch))\n",
    "plt.plot(epoch, lrate);\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def reset_weights(model):\n",
    "#     session = K.get_session()\n",
    "#     for layer in model.layers: \n",
    "#         if hasattr(layer, 'kernel_initializer'):\n",
    "#             layer.kernel.initializer.run(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cb_learning_rate = LearningRateScheduler(calc_learning_rate)\n",
    "cb_early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_model():\n",
    "    model = Sequential([\n",
    "        BatchNormalization(input_shape=(IMG_H, IMG_W, X_train.shape[-1]), axis=-1),\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(rate=0.3),\n",
    "        Flatten(),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def gen_model():\n",
    "#     model = Sequential([\n",
    "#         BatchNormalization(input_shape=(IMG_H, IMG_W, 4), axis=-1),\n",
    "#         Conv2D(filters=6, kernel_size=(5, 5), activation='relu', padding='same'),\n",
    "#         MaxPooling2D(pool_size=(2, 2), padding='valid'),\n",
    "#         Conv2D(filters=16, kernel_size=(5, 5), activation='relu', padding='valid'),\n",
    "#         MaxPooling2D(pool_size=(2, 2), padding='valid'),\n",
    "#         Flatten(),\n",
    "#         Dense(1024, activation='relu'),\n",
    "#         Dense(NUM_CLASSES, activation='softmax')\n",
    "#     ])\n",
    "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gen_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Programs\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image.py:1404: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (4000, 28, 28, 7) (7 channels).\n",
      "  str(self.x.shape[channels_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/250\n",
      "8/8 [==============================] - 3s 324ms/step - loss: 0.4720 - acc: 0.8312 - val_loss: 0.3897 - val_acc: 0.8613\n",
      "Epoch 52/250\n",
      "8/8 [==============================] - 3s 320ms/step - loss: 0.4854 - acc: 0.8268 - val_loss: 0.4081 - val_acc: 0.8627\n",
      "Epoch 53/250\n",
      "8/8 [==============================] - 3s 330ms/step - loss: 0.4819 - acc: 0.8293 - val_loss: 0.3838 - val_acc: 0.8587\n",
      "Epoch 54/250\n",
      "8/8 [==============================] - 3s 332ms/step - loss: 0.4600 - acc: 0.8380 - val_loss: 0.4171 - val_acc: 0.8440\n",
      "Epoch 55/250\n",
      "8/8 [==============================] - 3s 358ms/step - loss: 0.4810 - acc: 0.8308 - val_loss: 0.4359 - val_acc: 0.8480\n",
      "Epoch 56/250\n",
      "8/8 [==============================] - 3s 406ms/step - loss: 0.4729 - acc: 0.8253 - val_loss: 0.3632 - val_acc: 0.8600\n",
      "Epoch 57/250\n",
      "8/8 [==============================] - 3s 396ms/step - loss: 0.4822 - acc: 0.8293 - val_loss: 0.3806 - val_acc: 0.8627\n",
      "Epoch 58/250\n",
      "8/8 [==============================] - 3s 358ms/step - loss: 0.4733 - acc: 0.8248 - val_loss: 0.3624 - val_acc: 0.8680\n",
      "Epoch 59/250\n",
      "8/8 [==============================] - 3s 358ms/step - loss: 0.4501 - acc: 0.8402 - val_loss: 0.3679 - val_acc: 0.8693\n",
      "Epoch 60/250\n",
      "8/8 [==============================] - 3s 347ms/step - loss: 0.4379 - acc: 0.8430 - val_loss: 0.3643 - val_acc: 0.8733\n",
      "Epoch 61/250\n",
      "8/8 [==============================] - 3s 357ms/step - loss: 0.4648 - acc: 0.8335 - val_loss: 0.3806 - val_acc: 0.8573\n",
      "Epoch 62/250\n",
      "8/8 [==============================] - 3s 374ms/step - loss: 0.4704 - acc: 0.8307 - val_loss: 0.3663 - val_acc: 0.8680\n",
      "Epoch 63/250\n",
      "8/8 [==============================] - 3s 355ms/step - loss: 0.4703 - acc: 0.8347 - val_loss: 0.3710 - val_acc: 0.8573\n",
      "Epoch 64/250\n",
      "8/8 [==============================] - 3s 359ms/step - loss: 0.4332 - acc: 0.8427 - val_loss: 0.3508 - val_acc: 0.8693\n",
      "Epoch 65/250\n",
      "8/8 [==============================] - 3s 352ms/step - loss: 0.4493 - acc: 0.8425 - val_loss: 0.4169 - val_acc: 0.8387\n",
      "Epoch 66/250\n",
      "8/8 [==============================] - 3s 354ms/step - loss: 0.4467 - acc: 0.8342 - val_loss: 0.3873 - val_acc: 0.8520\n",
      "Epoch 67/250\n",
      "8/8 [==============================] - 3s 351ms/step - loss: 0.4352 - acc: 0.8450 - val_loss: 0.3599 - val_acc: 0.8600\n",
      "Epoch 68/250\n",
      "8/8 [==============================] - 3s 358ms/step - loss: 0.4262 - acc: 0.8480 - val_loss: 0.3351 - val_acc: 0.8707\n",
      "Epoch 69/250\n",
      "8/8 [==============================] - 3s 394ms/step - loss: 0.4243 - acc: 0.8513 - val_loss: 0.3524 - val_acc: 0.8733\n",
      "Epoch 70/250\n",
      "8/8 [==============================] - 3s 389ms/step - loss: 0.4117 - acc: 0.8495 - val_loss: 0.3281 - val_acc: 0.8827\n",
      "Epoch 71/250\n",
      "8/8 [==============================] - 3s 406ms/step - loss: 0.4338 - acc: 0.8435 - val_loss: 0.3468 - val_acc: 0.8733\n",
      "Epoch 72/250\n",
      "8/8 [==============================] - 3s 349ms/step - loss: 0.4133 - acc: 0.8515 - val_loss: 0.3578 - val_acc: 0.8707\n",
      "Epoch 73/250\n",
      "8/8 [==============================] - 3s 401ms/step - loss: 0.4198 - acc: 0.8455 - val_loss: 0.3414 - val_acc: 0.8760\n",
      "Epoch 74/250\n",
      "8/8 [==============================] - 3s 374ms/step - loss: 0.4029 - acc: 0.8490 - val_loss: 0.3505 - val_acc: 0.8720\n",
      "Epoch 75/250\n",
      "8/8 [==============================] - 3s 408ms/step - loss: 0.4031 - acc: 0.8592 - val_loss: 0.3354 - val_acc: 0.8773\n",
      "Epoch 76/250\n",
      "8/8 [==============================] - 3s 402ms/step - loss: 0.3976 - acc: 0.8568 - val_loss: 0.3178 - val_acc: 0.8880\n",
      "Epoch 77/250\n",
      "8/8 [==============================] - 3s 372ms/step - loss: 0.3903 - acc: 0.8645 - val_loss: 0.3544 - val_acc: 0.8653\n",
      "Epoch 78/250\n",
      "8/8 [==============================] - 3s 360ms/step - loss: 0.4025 - acc: 0.8613 - val_loss: 0.3181 - val_acc: 0.8853\n",
      "Epoch 79/250\n",
      "8/8 [==============================] - 3s 344ms/step - loss: 0.4057 - acc: 0.8505 - val_loss: 0.3281 - val_acc: 0.8853\n",
      "Epoch 80/250\n",
      "8/8 [==============================] - 3s 349ms/step - loss: 0.3974 - acc: 0.8602 - val_loss: 0.3158 - val_acc: 0.8893\n",
      "Epoch 81/250\n",
      "8/8 [==============================] - 3s 361ms/step - loss: 0.3757 - acc: 0.8672 - val_loss: 0.3314 - val_acc: 0.8853\n",
      "Epoch 82/250\n",
      "8/8 [==============================] - 3s 344ms/step - loss: 0.3994 - acc: 0.8562 - val_loss: 0.3142 - val_acc: 0.8907\n",
      "Epoch 83/250\n",
      "8/8 [==============================] - 3s 375ms/step - loss: 0.3771 - acc: 0.8643 - val_loss: 0.3502 - val_acc: 0.8680\n",
      "Epoch 84/250\n",
      "8/8 [==============================] - 3s 344ms/step - loss: 0.4049 - acc: 0.8550 - val_loss: 0.3116 - val_acc: 0.8920\n",
      "Epoch 85/250\n",
      "8/8 [==============================] - 3s 347ms/step - loss: 0.3993 - acc: 0.8547 - val_loss: 0.3210 - val_acc: 0.8867\n",
      "Epoch 86/250\n",
      "8/8 [==============================] - 3s 345ms/step - loss: 0.3850 - acc: 0.8600 - val_loss: 0.3323 - val_acc: 0.8773\n",
      "Epoch 87/250\n",
      "8/8 [==============================] - 3s 346ms/step - loss: 0.3854 - acc: 0.8585 - val_loss: 0.3189 - val_acc: 0.8867\n",
      "Epoch 88/250\n",
      "8/8 [==============================] - 3s 372ms/step - loss: 0.3692 - acc: 0.8698 - val_loss: 0.3157 - val_acc: 0.8920\n",
      "Epoch 89/250\n",
      "8/8 [==============================] - 3s 362ms/step - loss: 0.3717 - acc: 0.8670 - val_loss: 0.3328 - val_acc: 0.8787\n",
      "Epoch 90/250\n",
      "8/8 [==============================] - 3s 356ms/step - loss: 0.3693 - acc: 0.8710 - val_loss: 0.3401 - val_acc: 0.8787\n",
      "Epoch 91/250\n",
      "8/8 [==============================] - 3s 355ms/step - loss: 0.3698 - acc: 0.8653 - val_loss: 0.3100 - val_acc: 0.8880\n",
      "Epoch 92/250\n",
      "8/8 [==============================] - 3s 356ms/step - loss: 0.3608 - acc: 0.8692 - val_loss: 0.3353 - val_acc: 0.8760\n",
      "Epoch 93/250\n",
      "8/8 [==============================] - 3s 349ms/step - loss: 0.3535 - acc: 0.8773 - val_loss: 0.3105 - val_acc: 0.8920\n",
      "Epoch 94/250\n",
      "8/8 [==============================] - 3s 363ms/step - loss: 0.3770 - acc: 0.8673 - val_loss: 0.3346 - val_acc: 0.8800\n",
      "Epoch 95/250\n",
      "8/8 [==============================] - 3s 351ms/step - loss: 0.3817 - acc: 0.8675 - val_loss: 0.3437 - val_acc: 0.8640\n",
      "Epoch 96/250\n",
      "8/8 [==============================] - 3s 359ms/step - loss: 0.3737 - acc: 0.8708 - val_loss: 0.3207 - val_acc: 0.8827\n",
      "Epoch 97/250\n",
      "8/8 [==============================] - 3s 380ms/step - loss: 0.3781 - acc: 0.8640 - val_loss: 0.3245 - val_acc: 0.8867\n",
      "Epoch 98/250\n",
      "8/8 [==============================] - 3s 434ms/step - loss: 0.3654 - acc: 0.8640 - val_loss: 0.3123 - val_acc: 0.8827\n",
      "Epoch 99/250\n",
      "8/8 [==============================] - 3s 369ms/step - loss: 0.3569 - acc: 0.8727 - val_loss: 0.3052 - val_acc: 0.8907\n",
      "Epoch 100/250\n",
      "8/8 [==============================] - 3s 357ms/step - loss: 0.3552 - acc: 0.8708 - val_loss: 0.3079 - val_acc: 0.8893\n",
      "Epoch 101/250\n",
      "8/8 [==============================] - 3s 353ms/step - loss: 0.3543 - acc: 0.8695 - val_loss: 0.3123 - val_acc: 0.8813\n",
      "Epoch 102/250\n",
      "8/8 [==============================] - 3s 359ms/step - loss: 0.3458 - acc: 0.8745 - val_loss: 0.3204 - val_acc: 0.8933\n",
      "Epoch 103/250\n",
      "8/8 [==============================] - 3s 380ms/step - loss: 0.3458 - acc: 0.8805 - val_loss: 0.3055 - val_acc: 0.8920\n",
      "Epoch 104/250\n",
      "8/8 [==============================] - 3s 364ms/step - loss: 0.3438 - acc: 0.8690 - val_loss: 0.3060 - val_acc: 0.8920\n",
      "Epoch 105/250\n",
      "8/8 [==============================] - 3s 368ms/step - loss: 0.3320 - acc: 0.8808 - val_loss: 0.3094 - val_acc: 0.8893\n",
      "Epoch 106/250\n",
      "8/8 [==============================] - 3s 354ms/step - loss: 0.3606 - acc: 0.8708 - val_loss: 0.3097 - val_acc: 0.8960\n",
      "Epoch 107/250\n",
      "8/8 [==============================] - 3s 358ms/step - loss: 0.3440 - acc: 0.8683 - val_loss: 0.3061 - val_acc: 0.8920\n",
      "Epoch 108/250\n",
      "8/8 [==============================] - 3s 362ms/step - loss: 0.3420 - acc: 0.8723 - val_loss: 0.3180 - val_acc: 0.8813\n",
      "Epoch 109/250\n",
      "8/8 [==============================] - 3s 373ms/step - loss: 0.3456 - acc: 0.8810 - val_loss: 0.2950 - val_acc: 0.9013\n",
      "Epoch 110/250\n",
      "8/8 [==============================] - 3s 395ms/step - loss: 0.3471 - acc: 0.8745 - val_loss: 0.2931 - val_acc: 0.8933\n",
      "Epoch 111/250\n",
      "8/8 [==============================] - 7s 851ms/step - loss: 0.3342 - acc: 0.8770 - val_loss: 0.3261 - val_acc: 0.8867\n",
      "Epoch 112/250\n",
      "8/8 [==============================] - 4s 548ms/step - loss: 0.3420 - acc: 0.8763 - val_loss: 0.2898 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/250\n",
      "8/8 [==============================] - 4s 524ms/step - loss: 0.3416 - acc: 0.8788 - val_loss: 0.3104 - val_acc: 0.8933\n",
      "Epoch 114/250\n",
      "8/8 [==============================] - 4s 474ms/step - loss: 0.3271 - acc: 0.8810 - val_loss: 0.3068 - val_acc: 0.8933\n",
      "Epoch 115/250\n",
      "8/8 [==============================] - 3s 364ms/step - loss: 0.3369 - acc: 0.8750 - val_loss: 0.3016 - val_acc: 0.8920\n",
      "Epoch 116/250\n",
      "8/8 [==============================] - 3s 359ms/step - loss: 0.3405 - acc: 0.8762 - val_loss: 0.3144 - val_acc: 0.8840\n",
      "Epoch 117/250\n",
      "8/8 [==============================] - 3s 356ms/step - loss: 0.3361 - acc: 0.8713 - val_loss: 0.3093 - val_acc: 0.8933\n",
      "Epoch 118/250\n",
      "8/8 [==============================] - 3s 362ms/step - loss: 0.3367 - acc: 0.8810 - val_loss: 0.3067 - val_acc: 0.8920\n",
      "Epoch 119/250\n",
      "8/8 [==============================] - 3s 356ms/step - loss: 0.3288 - acc: 0.8820 - val_loss: 0.3019 - val_acc: 0.8920\n",
      "Epoch 120/250\n",
      "8/8 [==============================] - 3s 362ms/step - loss: 0.3240 - acc: 0.8878 - val_loss: 0.3098 - val_acc: 0.8920\n",
      "Epoch 121/250\n",
      "8/8 [==============================] - 3s 355ms/step - loss: 0.3150 - acc: 0.8815 - val_loss: 0.2982 - val_acc: 0.8960\n",
      "Epoch 122/250\n",
      "8/8 [==============================] - 3s 363ms/step - loss: 0.3193 - acc: 0.8810 - val_loss: 0.3088 - val_acc: 0.8893\n",
      "Epoch 123/250\n",
      "8/8 [==============================] - 3s 406ms/step - loss: 0.3255 - acc: 0.8773 - val_loss: 0.3081 - val_acc: 0.8880\n",
      "Epoch 124/250\n",
      "8/8 [==============================] - 3s 374ms/step - loss: 0.3191 - acc: 0.8838 - val_loss: 0.2932 - val_acc: 0.9000\n",
      "Epoch 125/250\n",
      "8/8 [==============================] - 3s 366ms/step - loss: 0.3182 - acc: 0.8832 - val_loss: 0.3110 - val_acc: 0.8813\n",
      "Epoch 126/250\n",
      "8/8 [==============================] - 4s 460ms/step - loss: 0.3220 - acc: 0.8827 - val_loss: 0.2968 - val_acc: 0.8880\n",
      "Epoch 127/250\n",
      "8/8 [==============================] - 6s 723ms/step - loss: 0.3325 - acc: 0.8842 - val_loss: 0.3050 - val_acc: 0.8840\n",
      "Epoch 128/250\n",
      "8/8 [==============================] - 3s 384ms/step - loss: 0.3263 - acc: 0.8820 - val_loss: 0.3053 - val_acc: 0.8920\n",
      "Epoch 129/250\n",
      "8/8 [==============================] - 3s 429ms/step - loss: 0.3146 - acc: 0.8818 - val_loss: 0.2938 - val_acc: 0.8907\n",
      "Epoch 130/250\n",
      "8/8 [==============================] - 3s 399ms/step - loss: 0.3229 - acc: 0.8820 - val_loss: 0.3029 - val_acc: 0.8960\n",
      "Epoch 131/250\n",
      "8/8 [==============================] - 3s 362ms/step - loss: 0.3237 - acc: 0.8848 - val_loss: 0.3132 - val_acc: 0.8813\n",
      "Epoch 132/250\n",
      "8/8 [==============================] - 3s 394ms/step - loss: 0.3214 - acc: 0.8858 - val_loss: 0.2968 - val_acc: 0.8907\n",
      "Epoch 133/250\n",
      "8/8 [==============================] - 3s 361ms/step - loss: 0.3268 - acc: 0.8795 - val_loss: 0.2966 - val_acc: 0.8920\n",
      "Epoch 134/250\n",
      "8/8 [==============================] - 3s 362ms/step - loss: 0.3201 - acc: 0.8790 - val_loss: 0.3046 - val_acc: 0.8893\n",
      "Epoch 135/250\n",
      "8/8 [==============================] - 4s 446ms/step - loss: 0.3241 - acc: 0.8858 - val_loss: 0.2991 - val_acc: 0.8947\n",
      "Epoch 136/250\n",
      "8/8 [==============================] - 3s 418ms/step - loss: 0.3264 - acc: 0.8848 - val_loss: 0.2945 - val_acc: 0.8987\n",
      "Epoch 137/250\n",
      "8/8 [==============================] - 3s 397ms/step - loss: 0.3318 - acc: 0.8813 - val_loss: 0.2968 - val_acc: 0.8893\n",
      "Epoch 138/250\n",
      "8/8 [==============================] - 3s 381ms/step - loss: 0.3220 - acc: 0.8793 - val_loss: 0.2971 - val_acc: 0.8947\n",
      "Epoch 139/250\n",
      "8/8 [==============================] - 4s 458ms/step - loss: 0.3219 - acc: 0.8808 - val_loss: 0.2994 - val_acc: 0.8933\n",
      "Epoch 140/250\n",
      "8/8 [==============================] - 3s 407ms/step - loss: 0.3329 - acc: 0.8780 - val_loss: 0.3052 - val_acc: 0.8867\n",
      "Epoch 141/250\n",
      "8/8 [==============================] - 3s 409ms/step - loss: 0.3198 - acc: 0.8847 - val_loss: 0.2935 - val_acc: 0.8933\n",
      "Epoch 142/250\n",
      "8/8 [==============================] - 5s 565ms/step - loss: 0.3215 - acc: 0.8833 - val_loss: 0.3041 - val_acc: 0.8933\n",
      "Epoch 143/250\n",
      "8/8 [==============================] - 3s 389ms/step - loss: 0.3375 - acc: 0.8740 - val_loss: 0.2974 - val_acc: 0.8920\n",
      "Epoch 144/250\n",
      "8/8 [==============================] - 3s 423ms/step - loss: 0.3179 - acc: 0.8872 - val_loss: 0.2949 - val_acc: 0.8907\n",
      "Epoch 145/250\n",
      "8/8 [==============================] - 3s 397ms/step - loss: 0.3230 - acc: 0.8825 - val_loss: 0.2943 - val_acc: 0.8973\n",
      "Epoch 146/250\n",
      "8/8 [==============================] - 3s 408ms/step - loss: 0.3221 - acc: 0.8845 - val_loss: 0.2981 - val_acc: 0.8933\n",
      "Epoch 147/250\n",
      "8/8 [==============================] - 3s 372ms/step - loss: 0.3288 - acc: 0.8817 - val_loss: 0.3029 - val_acc: 0.8907\n",
      "Epoch 148/250\n",
      "8/8 [==============================] - 3s 366ms/step - loss: 0.3161 - acc: 0.8878 - val_loss: 0.2979 - val_acc: 0.8907\n",
      "Epoch 149/250\n",
      "8/8 [==============================] - 3s 426ms/step - loss: 0.3286 - acc: 0.8840 - val_loss: 0.2957 - val_acc: 0.8933\n",
      "Epoch 150/250\n",
      "8/8 [==============================] - 4s 467ms/step - loss: 0.3311 - acc: 0.8800 - val_loss: 0.2995 - val_acc: 0.8920\n",
      "Epoch 151/250\n",
      "8/8 [==============================] - 3s 416ms/step - loss: 0.3243 - acc: 0.8825 - val_loss: 0.3063 - val_acc: 0.8853\n",
      "Epoch 152/250\n",
      "8/8 [==============================] - 4s 553ms/step - loss: 0.3169 - acc: 0.8843 - val_loss: 0.2985 - val_acc: 0.8960\n",
      "Epoch 153/250\n",
      "8/8 [==============================] - 4s 471ms/step - loss: 0.3327 - acc: 0.8797 - val_loss: 0.2927 - val_acc: 0.8933\n",
      "Epoch 154/250\n",
      "8/8 [==============================] - 3s 427ms/step - loss: 0.3057 - acc: 0.8855 - val_loss: 0.2949 - val_acc: 0.8893\n",
      "Epoch 155/250\n",
      "8/8 [==============================] - 4s 442ms/step - loss: 0.3294 - acc: 0.8768 - val_loss: 0.2947 - val_acc: 0.8893\n",
      "Epoch 156/250\n",
      "8/8 [==============================] - 3s 413ms/step - loss: 0.3255 - acc: 0.8802 - val_loss: 0.2986 - val_acc: 0.8947\n",
      "Epoch 157/250\n",
      "8/8 [==============================] - 3s 411ms/step - loss: 0.3133 - acc: 0.8848 - val_loss: 0.2947 - val_acc: 0.8933\n",
      "Epoch 158/250\n",
      "8/8 [==============================] - 3s 414ms/step - loss: 0.3185 - acc: 0.8910 - val_loss: 0.2980 - val_acc: 0.8933\n",
      "Epoch 159/250\n",
      "8/8 [==============================] - 3s 412ms/step - loss: 0.3096 - acc: 0.8858 - val_loss: 0.2993 - val_acc: 0.8920\n",
      "Epoch 160/250\n",
      "8/8 [==============================] - 3s 417ms/step - loss: 0.3116 - acc: 0.8890 - val_loss: 0.2912 - val_acc: 0.8933\n",
      "Epoch 161/250\n",
      "8/8 [==============================] - 3s 405ms/step - loss: 0.3051 - acc: 0.8880 - val_loss: 0.2904 - val_acc: 0.8933\n",
      "Epoch 162/250\n",
      "8/8 [==============================] - 3s 417ms/step - loss: 0.3036 - acc: 0.8903 - val_loss: 0.2950 - val_acc: 0.8947\n",
      "Epoch 163/250\n",
      "8/8 [==============================] - 3s 419ms/step - loss: 0.3082 - acc: 0.8848 - val_loss: 0.2950 - val_acc: 0.8933\n",
      "Epoch 164/250\n",
      "8/8 [==============================] - 4s 463ms/step - loss: 0.3110 - acc: 0.8893 - val_loss: 0.2939 - val_acc: 0.8920\n",
      "Epoch 165/250\n",
      "8/8 [==============================] - 3s 437ms/step - loss: 0.3121 - acc: 0.8887 - val_loss: 0.2966 - val_acc: 0.8867\n",
      "Epoch 166/250\n",
      "8/8 [==============================] - 4s 502ms/step - loss: 0.3272 - acc: 0.8848 - val_loss: 0.2940 - val_acc: 0.8933\n",
      "Epoch 167/250\n",
      "8/8 [==============================] - 3s 371ms/step - loss: 0.3275 - acc: 0.8785 - val_loss: 0.2918 - val_acc: 0.8987\n",
      "Epoch 168/250\n",
      "8/8 [==============================] - 3s 375ms/step - loss: 0.3153 - acc: 0.8855 - val_loss: 0.2928 - val_acc: 0.8987\n",
      "Epoch 169/250\n",
      "8/8 [==============================] - 3s 367ms/step - loss: 0.3152 - acc: 0.8857 - val_loss: 0.2948 - val_acc: 0.8947\n",
      "Epoch 170/250\n",
      "8/8 [==============================] - 3s 352ms/step - loss: 0.3175 - acc: 0.8832 - val_loss: 0.2961 - val_acc: 0.8947\n",
      "Epoch 171/250\n",
      "8/8 [==============================] - 3s 352ms/step - loss: 0.3237 - acc: 0.8807 - val_loss: 0.2969 - val_acc: 0.8960\n",
      "Epoch 172/250\n",
      "8/8 [==============================] - 3s 362ms/step - loss: 0.3089 - acc: 0.8828 - val_loss: 0.2957 - val_acc: 0.8960\n",
      "Epoch 173/250\n",
      "8/8 [==============================] - 3s 346ms/step - loss: 0.3062 - acc: 0.8840 - val_loss: 0.2933 - val_acc: 0.8973\n",
      "Epoch 174/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 3s 351ms/step - loss: 0.3183 - acc: 0.8825 - val_loss: 0.2931 - val_acc: 0.8960\n",
      "Epoch 175/250\n",
      "8/8 [==============================] - 3s 374ms/step - loss: 0.3045 - acc: 0.8893 - val_loss: 0.2909 - val_acc: 0.8947\n",
      "Epoch 176/250\n",
      "8/8 [==============================] - 3s 357ms/step - loss: 0.3111 - acc: 0.8843 - val_loss: 0.2927 - val_acc: 0.8947\n",
      "Epoch 177/250\n",
      "8/8 [==============================] - 3s 367ms/step - loss: 0.3156 - acc: 0.8845 - val_loss: 0.2965 - val_acc: 0.8960\n",
      "Epoch 178/250\n",
      "8/8 [==============================] - 3s 362ms/step - loss: 0.3078 - acc: 0.8893 - val_loss: 0.2947 - val_acc: 0.8987\n",
      "Epoch 179/250\n",
      "8/8 [==============================] - 3s 376ms/step - loss: 0.3205 - acc: 0.8888 - val_loss: 0.2944 - val_acc: 0.8947\n",
      "Epoch 180/250\n",
      "8/8 [==============================] - 3s 375ms/step - loss: 0.3173 - acc: 0.8855 - val_loss: 0.2953 - val_acc: 0.8933\n",
      "Epoch 181/250\n",
      "8/8 [==============================] - 4s 445ms/step - loss: 0.3152 - acc: 0.8837 - val_loss: 0.2959 - val_acc: 0.8920\n",
      "Epoch 182/250\n",
      "8/8 [==============================] - 3s 414ms/step - loss: 0.3193 - acc: 0.8808 - val_loss: 0.2967 - val_acc: 0.8920\n",
      "Epoch 183/250\n",
      "8/8 [==============================] - 3s 383ms/step - loss: 0.3051 - acc: 0.8867 - val_loss: 0.2963 - val_acc: 0.8933\n",
      "Epoch 184/250\n",
      "8/8 [==============================] - 3s 370ms/step - loss: 0.3169 - acc: 0.8868 - val_loss: 0.2971 - val_acc: 0.8947\n",
      "Epoch 185/250\n",
      "8/8 [==============================] - 3s 392ms/step - loss: 0.2993 - acc: 0.8920 - val_loss: 0.3011 - val_acc: 0.8907\n",
      "Epoch 186/250\n",
      "8/8 [==============================] - 3s 390ms/step - loss: 0.3092 - acc: 0.8882 - val_loss: 0.2978 - val_acc: 0.8973\n",
      "Epoch 187/250\n",
      "8/8 [==============================] - 3s 402ms/step - loss: 0.3021 - acc: 0.8883 - val_loss: 0.2961 - val_acc: 0.8933\n",
      "Epoch 188/250\n",
      "8/8 [==============================] - 3s 389ms/step - loss: 0.3160 - acc: 0.8867 - val_loss: 0.2952 - val_acc: 0.8960\n",
      "Epoch 189/250\n",
      "8/8 [==============================] - 3s 398ms/step - loss: 0.3024 - acc: 0.8890 - val_loss: 0.2949 - val_acc: 0.9000\n",
      "Epoch 190/250\n",
      "8/8 [==============================] - 3s 370ms/step - loss: 0.3081 - acc: 0.8912 - val_loss: 0.2943 - val_acc: 0.8973\n",
      "Epoch 191/250\n",
      "8/8 [==============================] - 3s 406ms/step - loss: 0.3196 - acc: 0.8837 - val_loss: 0.2935 - val_acc: 0.8960\n",
      "Epoch 192/250\n",
      "8/8 [==============================] - 3s 388ms/step - loss: 0.3110 - acc: 0.8865 - val_loss: 0.2958 - val_acc: 0.8947\n",
      "Epoch 193/250\n",
      "8/8 [==============================] - 3s 365ms/step - loss: 0.3081 - acc: 0.8877 - val_loss: 0.2974 - val_acc: 0.8933\n",
      "Epoch 194/250\n",
      "8/8 [==============================] - 3s 358ms/step - loss: 0.3097 - acc: 0.8895 - val_loss: 0.2966 - val_acc: 0.8960\n",
      "Epoch 195/250\n",
      "8/8 [==============================] - 3s 352ms/step - loss: 0.3097 - acc: 0.8865 - val_loss: 0.2963 - val_acc: 0.8960\n",
      "Epoch 196/250\n",
      "8/8 [==============================] - 3s 349ms/step - loss: 0.3108 - acc: 0.8907 - val_loss: 0.2956 - val_acc: 0.8973\n",
      "Epoch 197/250\n",
      "8/8 [==============================] - 3s 351ms/step - loss: 0.3212 - acc: 0.8863 - val_loss: 0.2941 - val_acc: 0.9000\n",
      "Epoch 198/250\n",
      "8/8 [==============================] - 3s 356ms/step - loss: 0.3098 - acc: 0.8865 - val_loss: 0.2934 - val_acc: 0.8960\n",
      "Epoch 199/250\n",
      "8/8 [==============================] - 3s 348ms/step - loss: 0.3084 - acc: 0.8885 - val_loss: 0.2937 - val_acc: 0.8960\n",
      "Epoch 200/250\n",
      "8/8 [==============================] - 3s 351ms/step - loss: 0.3220 - acc: 0.8840 - val_loss: 0.2948 - val_acc: 0.8960\n",
      "Epoch 201/250\n",
      "8/8 [==============================] - 3s 357ms/step - loss: 0.3098 - acc: 0.8845 - val_loss: 0.2981 - val_acc: 0.8960\n",
      "Epoch 202/250\n",
      "8/8 [==============================] - 3s 350ms/step - loss: 0.3161 - acc: 0.8828 - val_loss: 0.3010 - val_acc: 0.8960\n",
      "Epoch 203/250\n",
      "8/8 [==============================] - 3s 352ms/step - loss: 0.3218 - acc: 0.8838 - val_loss: 0.2991 - val_acc: 0.8960\n",
      "Epoch 204/250\n",
      "8/8 [==============================] - 3s 351ms/step - loss: 0.3139 - acc: 0.8818 - val_loss: 0.2966 - val_acc: 0.8947\n",
      "Epoch 205/250\n",
      "8/8 [==============================] - 3s 349ms/step - loss: 0.2991 - acc: 0.8898 - val_loss: 0.2950 - val_acc: 0.8960\n",
      "Epoch 206/250\n",
      "8/8 [==============================] - 3s 348ms/step - loss: 0.3099 - acc: 0.8840 - val_loss: 0.2956 - val_acc: 0.8960\n",
      "Epoch 207/250\n",
      "8/8 [==============================] - 3s 351ms/step - loss: 0.3169 - acc: 0.8892 - val_loss: 0.2976 - val_acc: 0.8933\n",
      "Epoch 208/250\n",
      "8/8 [==============================] - 3s 348ms/step - loss: 0.3104 - acc: 0.8920 - val_loss: 0.2981 - val_acc: 0.8933\n",
      "Epoch 209/250\n",
      "8/8 [==============================] - 3s 351ms/step - loss: 0.3067 - acc: 0.8895 - val_loss: 0.2975 - val_acc: 0.8960\n",
      "Epoch 210/250\n",
      "8/8 [==============================] - 3s 356ms/step - loss: 0.3076 - acc: 0.8903 - val_loss: 0.2975 - val_acc: 0.8960\n",
      "Epoch 211/250\n",
      "8/8 [==============================] - 3s 358ms/step - loss: 0.3108 - acc: 0.8890 - val_loss: 0.2991 - val_acc: 0.8947\n",
      "Epoch 212/250\n",
      "8/8 [==============================] - 3s 347ms/step - loss: 0.3159 - acc: 0.8872 - val_loss: 0.2977 - val_acc: 0.8920\n",
      "Epoch 213/250\n",
      "8/8 [==============================] - 3s 356ms/step - loss: 0.3103 - acc: 0.8865 - val_loss: 0.2962 - val_acc: 0.8973\n",
      "Epoch 214/250\n",
      "8/8 [==============================] - 3s 352ms/step - loss: 0.3087 - acc: 0.8855 - val_loss: 0.2961 - val_acc: 0.8960\n",
      "Epoch 215/250\n",
      "8/8 [==============================] - 3s 350ms/step - loss: 0.3135 - acc: 0.8845 - val_loss: 0.2955 - val_acc: 0.8933\n",
      "Epoch 216/250\n",
      "8/8 [==============================] - 3s 369ms/step - loss: 0.3033 - acc: 0.8865 - val_loss: 0.2969 - val_acc: 0.8920\n",
      "Epoch 217/250\n",
      "8/8 [==============================] - 3s 359ms/step - loss: 0.2999 - acc: 0.8940 - val_loss: 0.2979 - val_acc: 0.8907\n",
      "Epoch 218/250\n",
      "8/8 [==============================] - 3s 351ms/step - loss: 0.3177 - acc: 0.8888 - val_loss: 0.2980 - val_acc: 0.8907\n",
      "Epoch 219/250\n",
      "8/8 [==============================] - 3s 352ms/step - loss: 0.3163 - acc: 0.8813 - val_loss: 0.2983 - val_acc: 0.8960\n",
      "Epoch 220/250\n",
      "8/8 [==============================] - 3s 355ms/step - loss: 0.3000 - acc: 0.8907 - val_loss: 0.2971 - val_acc: 0.8947\n",
      "Epoch 221/250\n",
      "8/8 [==============================] - 3s 355ms/step - loss: 0.3177 - acc: 0.8890 - val_loss: 0.2970 - val_acc: 0.8960\n",
      "Epoch 222/250\n",
      "8/8 [==============================] - 3s 373ms/step - loss: 0.3110 - acc: 0.8895 - val_loss: 0.2967 - val_acc: 0.8973\n",
      "Epoch 223/250\n",
      "8/8 [==============================] - 3s 369ms/step - loss: 0.3196 - acc: 0.8840 - val_loss: 0.2977 - val_acc: 0.8933\n",
      "Epoch 224/250\n",
      "8/8 [==============================] - 3s 418ms/step - loss: 0.3160 - acc: 0.8835 - val_loss: 0.2978 - val_acc: 0.8947\n",
      "Epoch 225/250\n",
      "8/8 [==============================] - 3s 364ms/step - loss: 0.3153 - acc: 0.8930 - val_loss: 0.2979 - val_acc: 0.8960\n",
      "Epoch 226/250\n",
      "8/8 [==============================] - 3s 347ms/step - loss: 0.3147 - acc: 0.8858 - val_loss: 0.2977 - val_acc: 0.8987\n",
      "Epoch 227/250\n",
      "8/8 [==============================] - 3s 350ms/step - loss: 0.3075 - acc: 0.8875 - val_loss: 0.2965 - val_acc: 0.8987\n",
      "Epoch 228/250\n",
      "8/8 [==============================] - 3s 357ms/step - loss: 0.3172 - acc: 0.8803 - val_loss: 0.2954 - val_acc: 0.8987\n",
      "Epoch 229/250\n",
      "8/8 [==============================] - 3s 347ms/step - loss: 0.3163 - acc: 0.8905 - val_loss: 0.2956 - val_acc: 0.8987\n",
      "Epoch 230/250\n",
      "8/8 [==============================] - 3s 350ms/step - loss: 0.3125 - acc: 0.8895 - val_loss: 0.2959 - val_acc: 0.8987\n",
      "Epoch 231/250\n",
      "8/8 [==============================] - 3s 352ms/step - loss: 0.3107 - acc: 0.8842 - val_loss: 0.2959 - val_acc: 0.8987\n",
      "Epoch 232/250\n",
      "8/8 [==============================] - 3s 380ms/step - loss: 0.3018 - acc: 0.8910 - val_loss: 0.2956 - val_acc: 0.8987\n",
      "Epoch 233/250\n",
      "8/8 [==============================] - 3s 350ms/step - loss: 0.3090 - acc: 0.8888 - val_loss: 0.2958 - val_acc: 0.8987\n",
      "Epoch 234/250\n",
      "8/8 [==============================] - 3s 354ms/step - loss: 0.3002 - acc: 0.8873 - val_loss: 0.2959 - val_acc: 0.8987\n",
      "Epoch 235/250\n",
      "8/8 [==============================] - 3s 397ms/step - loss: 0.2955 - acc: 0.8920 - val_loss: 0.2964 - val_acc: 0.8987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236/250\n",
      "8/8 [==============================] - 3s 355ms/step - loss: 0.3000 - acc: 0.8903 - val_loss: 0.2966 - val_acc: 0.8960\n",
      "Epoch 237/250\n",
      "8/8 [==============================] - 3s 339ms/step - loss: 0.3221 - acc: 0.8827 - val_loss: 0.2966 - val_acc: 0.8947\n",
      "Epoch 238/250\n",
      "8/8 [==============================] - 3s 347ms/step - loss: 0.3218 - acc: 0.8830 - val_loss: 0.2969 - val_acc: 0.8947\n",
      "Epoch 239/250\n",
      "8/8 [==============================] - 3s 364ms/step - loss: 0.3230 - acc: 0.8803 - val_loss: 0.2966 - val_acc: 0.8973\n",
      "Epoch 240/250\n",
      "8/8 [==============================] - 3s 404ms/step - loss: 0.2999 - acc: 0.8930 - val_loss: 0.2964 - val_acc: 0.8973\n",
      "Epoch 241/250\n",
      "8/8 [==============================] - 3s 351ms/step - loss: 0.2969 - acc: 0.8942 - val_loss: 0.2969 - val_acc: 0.8973\n",
      "Epoch 242/250\n",
      "8/8 [==============================] - 3s 379ms/step - loss: 0.3091 - acc: 0.8928 - val_loss: 0.2972 - val_acc: 0.8973\n",
      "Epoch 243/250\n",
      "8/8 [==============================] - 3s 344ms/step - loss: 0.3110 - acc: 0.8875 - val_loss: 0.2973 - val_acc: 0.8960\n",
      "Epoch 244/250\n",
      "8/8 [==============================] - 3s 350ms/step - loss: 0.3077 - acc: 0.8855 - val_loss: 0.2974 - val_acc: 0.8960\n",
      "Epoch 245/250\n",
      "8/8 [==============================] - 3s 344ms/step - loss: 0.3163 - acc: 0.8825 - val_loss: 0.2977 - val_acc: 0.8933\n",
      "Epoch 246/250\n",
      "8/8 [==============================] - 3s 354ms/step - loss: 0.3118 - acc: 0.8870 - val_loss: 0.2977 - val_acc: 0.8933\n",
      "Epoch 247/250\n",
      "8/8 [==============================] - 3s 374ms/step - loss: 0.3066 - acc: 0.8885 - val_loss: 0.2971 - val_acc: 0.8960\n",
      "Epoch 248/250\n",
      "8/8 [==============================] - 3s 348ms/step - loss: 0.3077 - acc: 0.8910 - val_loss: 0.2969 - val_acc: 0.8960\n",
      "Epoch 249/250\n",
      "8/8 [==============================] - 3s 355ms/step - loss: 0.3008 - acc: 0.8888 - val_loss: 0.2963 - val_acc: 0.8973\n",
      "Epoch 250/250\n",
      "8/8 [==============================] - 3s 361ms/step - loss: 0.3032 - acc: 0.8965 - val_loss: 0.2959 - val_acc: 0.8987\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(r'0.88539\\model_03.weights.h5')\n",
    "\n",
    "history = model.fit_generator(\n",
    "    aug_generator.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n",
    "    validation_data = (X_valid, Y_valid),\n",
    "    epochs = NUM_EPOCHS + 50,\n",
    "    steps_per_epoch = X_train.shape[0] // BATCH_SIZE,\n",
    "    # callbacks = [cb_learning_rate, cb_early_stopping],\n",
    "    callbacks = [cb_learning_rate],\n",
    "    verbose=1,\n",
    "    initial_epoch=50\n",
    ")\n",
    "\n",
    "result = np.argmax(model.predict(X_test), axis=1)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# glob('test\\*.png')[0].split('\\\\')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = [path.split('\\\\')[-1] for path in glob('test\\*.png')]\n",
    "submit = pd.DataFrame({'file': files, 'species': le.inverse_transform(result)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# submissions = pd.DataFrame({'ImageId': list(range(1, len(results1) + 1)), 'Label': results1})\n",
    "# submissions.to_csv('kaggle_keras_09.2.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit.to_csv('model_05.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open('model_05.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights('model_05.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
