{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "#from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('application_train.csv')\n",
    "test_df = pd.read_csv('application_test.csv')\n",
    "cc = pd.read_csv('credit_card_balance.csv')\n",
    "pos = pd.read_csv('POS_CASH_balance.csv')\n",
    "ins = pd.read_csv('installments_payments.csv')\n",
    "bureau = pd.read_csv('bureau.csv')\n",
    "bb = pd.read_csv('bureau_balance.csv')\n",
    "prev = pd.read_csv('previous_application.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 307511, test samples: 48744\n",
      "Credit Card: 3840312, POS: 10001358, installments samples: 13605401\n",
      "Bureau info: 1716428, balance: 27299925\n"
     ]
    }
   ],
   "source": [
    "print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
    "print(\"Credit Card: {}, POS: {}, installments samples: {}\".format(len(cc), len(pos), len(ins)))\n",
    "print(\"Bureau info: {}, balance: {}\".format(len(bureau), len(bb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('application_train.csv')\n",
    "test_df = pd.read_csv('application_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(df):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(test_df).reset_index()\n",
    "df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
    "df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "\n",
    "# Some simple new features (percentages)\n",
    "df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "\n",
    "#Создание признаков для следующей категоризации\n",
    "df['DAYS_EMPLOYED']=abs(df['DAYS_EMPLOYED']//365)\n",
    "df['DAYS_REGISTRATION']=abs(df['DAYS_REGISTRATION']//365)\n",
    "df['DAYS_ID_PUBLISH']=abs(df['DAYS_ID_PUBLISH']//365)\n",
    "df['age']=abs(df['DAYS_BIRTH']//365)\n",
    "df['phone']=abs(df['DAYS_LAST_PHONE_CHANGE']//365)\n",
    "\n",
    "# additional features\n",
    "df['NEW_CREDIT_TO_ANNUITY_RATIO'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n",
    "df['NEW_CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
    "df['NEW_INC_PER_CHLD'] = df['AMT_INCOME_TOTAL'] / (1 + df['CNT_CHILDREN'])\n",
    "df['NEW_EMPLOY_TO_BIRTH_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "df['NEW_ANNUITY_TO_INCOME_RATIO'] = df['AMT_ANNUITY'] / (1 + df['AMT_INCOME_TOTAL'])\n",
    "df['NEW_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
    "df['NEW_EXT_SOURCES_MEAN'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "df['NEW_SCORES_STD'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
    "df['NEW_SCORES_STD'] = df['NEW_SCORES_STD'].fillna(df['NEW_SCORES_STD'].mean())\n",
    "df['NEW_CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_BIRTH']\n",
    "df['NEW_CAR_TO_EMPLOY_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']\n",
    "df['NEW_PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']\n",
    "df['NEW_PHONE_TO_EMPLOY_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_EMPLOYED']\n",
    "df['NEW_CREDIT_TO_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n",
    "\n",
    "\n",
    "\n",
    "docs = [_f for _f in df.columns if 'FLAG_DOC' in _f]\n",
    "live = [_f for _f in df.columns if ('FLAG_' in _f) & ('FLAG_DOC' not in _f) & ('_FLAG_' not in _f)]\n",
    "inc_by_org = df[['AMT_INCOME_TOTAL', 'ORGANIZATION_TYPE']].groupby('ORGANIZATION_TYPE').median()['AMT_INCOME_TOTAL']\n",
    "k = df[['AMT_INCOME_TOTAL','REGION_POPULATION_RELATIVE']].groupby('REGION_POPULATION_RELATIVE').median()['AMT_INCOME_TOTAL']\n",
    "\n",
    "df['REGION_POPULATION_RELATIVE1']=df['REGION_POPULATION_RELATIVE']*1000\n",
    "df['NEW_INC_BY_RER'] = df['REGION_POPULATION_RELATIVE'].map(k)\n",
    "df['NEW_INC_BY_ORG'] = df['ORGANIZATION_TYPE'].map(inc_by_org)\n",
    "df['NEW_DOC_IND_AVG'] = df[docs].mean(axis=1)\n",
    "df['NEW_DOC_IND_STD'] = df[docs].std(axis=1)\n",
    "df['NEW_DOC_IND_KURT'] = df[docs].kurtosis(axis=1)\n",
    "df['NEW_LIVE_IND_SUM'] = df[live].sum(axis=1)\n",
    "df['NEW_LIVE_IND_STD'] = df[live].std(axis=1)\n",
    "df['NEW_LIVE_IND_KURT'] = df[live].kurtosis(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phone_group (row):\n",
    "    if 0<= row['phone'] <= 1:\n",
    "        return 0\n",
    "\n",
    "    if 1<= row['phone'] <= 2:\n",
    "        return 1\n",
    "\n",
    "    if 2<= row['phone'] <= 3:\n",
    "        return 2\n",
    "\n",
    "    if 3<= row['phone'] <= 5: \n",
    "        return 3\n",
    "    \n",
    "    if 5<= row['phone'] <= 13:\n",
    "        return 4\n",
    "\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "\n",
    "def family_group (row):\n",
    "    if  row['CNT_FAM_MEMBERS'] == 0:\n",
    "        return '0'\n",
    "\n",
    "    if row['CNT_FAM_MEMBERS'] == 1:\n",
    "        return '1'\n",
    "\n",
    "    if row['CNT_FAM_MEMBERS'] == 2:\n",
    "        return '2'\n",
    "\n",
    "    if row['CNT_FAM_MEMBERS'] == 3 : \n",
    "        return '3'\n",
    "    \n",
    "    if row['CNT_FAM_MEMBERS'] == 4:\n",
    "        return '4'\n",
    "\n",
    "    else:\n",
    "        return 'vibrosi'\n",
    "\n",
    "def region (row):\n",
    "    if 0<= row['REGION_POPULATION_RELATIVE1'] < 5:\n",
    "        return 'very_lo'\n",
    "\n",
    "    if 5<= row['REGION_POPULATION_RELATIVE1'] < 10:\n",
    "        return 'lo'\n",
    "\n",
    "    if 10<= row['REGION_POPULATION_RELATIVE1'] <= 20: \n",
    "        return 'mid'\n",
    "    \n",
    "    if 20<= row['REGION_POPULATION_RELATIVE1'] <= 40:\n",
    "        return 'hi_mid'\n",
    "\n",
    "    if 40<= row['REGION_POPULATION_RELATIVE1'] <= 70: \n",
    "        return 'hi'\n",
    "    \n",
    "    else:\n",
    "        return 'very_high'\n",
    "    \n",
    "def age_group (row):\n",
    "    if 18<= row['age'] <= 22:\n",
    "        return 'very_young'\n",
    "\n",
    "    if 23<= row['age'] <= 30:\n",
    "        return 'young'\n",
    "\n",
    "    if 30<= row['age'] <= 40:\n",
    "        return 'older_young'\n",
    "\n",
    "    if 40<= row['age'] <= 55: \n",
    "        return 'adult'\n",
    "    \n",
    "    if 55<= row['age'] <= 85: \n",
    "        return 'super_puper_grand'\n",
    "    \n",
    "    else:\n",
    "        return 'vibrosi'\n",
    "    \n",
    "def Employ (row):\n",
    "    if 0<= row['DAYS_EMPLOYED'] <= 0.5:\n",
    "        return 'malo'\n",
    "\n",
    "    if 0.5<= row['DAYS_EMPLOYED'] <= 1:\n",
    "        return 'young1'\n",
    "\n",
    "    if 1<= row['DAYS_EMPLOYED'] <= 2:\n",
    "        return 'older_young1'\n",
    "\n",
    "    if 2<= row['DAYS_EMPLOYED'] <= 5: \n",
    "        return 'adult1'\n",
    "    \n",
    "    if 5<= row['DAYS_EMPLOYED'] <= 13:\n",
    "        return 'mnogo'\n",
    "\n",
    "    else:\n",
    "        return 'vibrosi'\n",
    "\n",
    "\n",
    "def REGISTR (row):\n",
    "    if 0<= row['DAYS_REGISTRATION'] <= 1:\n",
    "        return 'very_young2'\n",
    "\n",
    "    if 1<= row['DAYS_REGISTRATION'] <= 2:\n",
    "        return 'young2'\n",
    "\n",
    "    if 2<= row['DAYS_REGISTRATION'] <= 5:\n",
    "        return 'older_young2'\n",
    "\n",
    "    if 5<= row['DAYS_REGISTRATION'] <= 7: \n",
    "        return 'adult2'\n",
    "    \n",
    "    if 7<= row['DAYS_REGISTRATION'] <= 13:\n",
    "        return 'adult_plus2'\n",
    "\n",
    "    else:\n",
    "        return 'vibrosi2'\n",
    "    \n",
    "def ID_PUBLISH (row):\n",
    "    if 0<= row['DAYS_ID_PUBLISH'] <= 1:\n",
    "        return 'very_young3'\n",
    "\n",
    "    if 1<= row['DAYS_ID_PUBLISH'] <= 3:\n",
    "        return 'young3'\n",
    "\n",
    "    if 3<= row['DAYS_ID_PUBLISH'] <= 5:\n",
    "        return 'older_young3'\n",
    "\n",
    "    if 5<= row['DAYS_ID_PUBLISH'] <= 7: \n",
    "        return 'adult3'\n",
    "    \n",
    "    if 7<= row['DAYS_ID_PUBLISH'] <= 13:\n",
    "        return 'adult_plus3'\n",
    "\n",
    "    else:\n",
    "        return 'vibrosi3'\n",
    "    \n",
    "def IsWeekend (row):\n",
    "    if row['WEEKDAY_APPR_PROCESS_START'] == 'SATURDAY':\n",
    "        return 1\n",
    "    \n",
    "    if row['WEEKDAY_APPR_PROCESS_START'] == 'SANDAY':\n",
    "        return 1\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def education (row): \n",
    "    if row['NAME_EDUCATION_TYPE'] == 'Higher education': \n",
    "        return 1\n",
    "    if row['NAME_EDUCATION_TYPE'] == 'Academic degree': \n",
    "        return 1       \n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "def family_stat (row): \n",
    "    if row['NAME_FAMILY_STATUS'] == 'Married': \n",
    "        return 1\n",
    "    if row['NAME_FAMILY_STATUS'] == 'Civil marriage': \n",
    "        return 1\n",
    "   \n",
    "    else: \n",
    "        return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['w_h']=df['WEEKDAY_APPR_PROCESS_START'] + str(df['HOUR_APPR_PROCESS_START'])\n",
    "\n",
    "#df['phone_group'] = df.apply(phone_group, axis=1)\n",
    "#df['family_group'] = df.apply(family_group, axis=1)\n",
    "#df['region_group'] = df.apply(region, axis=1)\n",
    "#df['age_group'] = df.apply(age_group, axis=1)\n",
    "#df['age_group'] = df.apply(age_group, axis=1)\n",
    "#df['Employ'] = df.apply(Employ, axis=1)\n",
    "#df['Registr'] = df.apply(REGISTR, axis=1)\n",
    "#df['ID_publish'] = df.apply(ID_PUBLISH, axis=1)\n",
    "#df['IsWeekend'] = df.apply(IsWeekend, axis=1)\n",
    "#df['bolshe']=df.apply(asf, axis=1)\n",
    "#df['car'] = df.apply(car, axis=1)\n",
    "#df['realty']=df.apply(realty, axis=1)\n",
    "#df['gender']=df.apply(gender, axis=1)\n",
    "df['education']=df.apply(education, axis=1)\n",
    "df['family_stat']=df.apply(family_stat, axis=1)\n",
    "\n",
    "df['ext1'] = df['EXT_SOURCE_1'] + df['EXT_SOURCE_2'] +df['EXT_SOURCE_3']\n",
    "df['ext2'] = (df['EXT_SOURCE_1'] + df['EXT_SOURCE_2'] +df['EXT_SOURCE_3'])/3\n",
    "df['ext3'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3'] \n",
    "df['ext4'] = df['EXT_SOURCE_1'] /df['EXT_SOURCE_2'] /df['EXT_SOURCE_3'] \n",
    "df['ext5'] = df['EXT_SOURCE_1'] *df['EXT_SOURCE_2'] /df['EXT_SOURCE_3'] \n",
    "df['ext6'] = df['EXT_SOURCE_1'] +df['EXT_SOURCE_2'] -df['EXT_SOURCE_3'] \n",
    "df['ext7'] = df['EXT_SOURCE_1'] -df['EXT_SOURCE_2'] +df['EXT_SOURCE_3'] \n",
    "df['ext8'] = df['EXT_SOURCE_1'] *df['EXT_SOURCE_2']\n",
    "df['ext9'] = df['EXT_SOURCE_1'] *df['EXT_SOURCE_3']\n",
    "df['ext10'] = df['EXT_SOURCE_3'] *df['EXT_SOURCE_2']\n",
    "df['ext11'] = df['EXT_SOURCE_1']**2\n",
    "df['ext12'] = df['EXT_SOURCE_2']**2\n",
    "df['ext13'] = df['EXT_SOURCE_3']**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without_history=df[df['EXT_SOURCE_1'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b=without_history[without_history['EXT_SOURCE_3'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cat_feat = list(df.dtypes[df.dtypes == object].index)\n",
    "num_feat = [f for f in df if f not in cat_feat ]\n",
    "\n",
    "def get_woe_v1(df_train, df_test, col, target_col):\n",
    "    all_good = len(df_train[df_train[target_col] == 1][col])\n",
    "    all_bad = len(df_train[df_train[target_col] == 0][col])\n",
    "    odds_series = (\n",
    "        df_train[df_train[target_col] == 1][col].value_counts()\n",
    "        /\n",
    "        df_train[df_train[target_col] == 0][col].value_counts()\n",
    "    )\n",
    "    odds_series = odds_series / all_good * all_bad\n",
    "    category_woe_dict = np.log(odds_series).to_dict()\n",
    "    df_train[col + '_woe'] = df_train[col].apply(category_woe_dict.get)\n",
    "    df_test[col + '_woe'] = df_test[col].apply(category_woe_dict.get)\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "for cat_feat in cat_feat:\n",
    "    data_train, data_test = get_woe_v1(train, test, cat_feat, 'TARGET')\n",
    "    \n",
    "cat_feat = list(data_train.dtypes[data_train.dtypes == object].index)\n",
    "num_feat = [f for f in train if f not in cat_feat ]\n",
    "\n",
    "df=data_train.drop(cat_feat, axis=1)\n",
    "df=data_test.drop(cat_feat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train123.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical features with One-Hot encode\n",
    "#df, cat_cols = one_hot_encoder(df)\n",
    "# Categorical features with Binary encode (0 or 1; two categories)\n",
    "#for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    " #   df[bin_feature], uniques = pd.factorize(df[bin_feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "del prev['AMT_DOWN_PAYMENT']\n",
    "del prev['RATE_INTEREST_PRIMARY']\n",
    "del prev['RATE_INTEREST_PRIVILEGED']\n",
    "del prev['NAME_TYPE_SUITE']\n",
    "del prev['DAYS_FIRST_DRAWING']\n",
    "del prev['DAYS_TERMINATION']\n",
    "del prev['DAYS_DECISION']\n",
    "del prev['HOUR_APPR_PROCESS_START']\n",
    "del prev['SELLERPLACE_AREA']\n",
    "del prev['NAME_PRODUCT_TYPE']\n",
    "del prev['CHANNEL_TYPE']\n",
    "del prev['NAME_PAYMENT_TYPE']\n",
    "del prev['PRODUCT_COMBINATION']\n",
    "del prev['AMT_GOODS_PRICE']\n",
    "del prev['NFLAG_LAST_APPL_IN_DAY']\n",
    "del prev['NAME_CASH_LOAN_PURPOSE']\n",
    "del prev['NAME_SELLER_INDUSTRY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GOOD_CAT (row):        \n",
    "    if row['NAME_GOODS_CATEGORY'] == 'Mobile':\n",
    "        return 'Mobile'   \n",
    "    if row['NAME_GOODS_CATEGORY'] == 'Consumer Electronics':\n",
    "        return 'Electronics'\n",
    "    if row['NAME_GOODS_CATEGORY'] == 'Photo / Cinema Equipment':\n",
    "        return 'Electronics'\n",
    "    if row['NAME_GOODS_CATEGORY'] == 'Computers':\n",
    "        return 'Electronics'\n",
    "    if row['NAME_GOODS_CATEGORY'] == 'Audio/Video':\n",
    "        return 'Electronics'     \n",
    "    if row['NAME_GOODS_CATEGORY'] == 'Construction Materials':\n",
    "        return '3'  \n",
    "    if row['NAME_GOODS_CATEGORY'] == 'Furniture':\n",
    "        return '3'\n",
    "    if row['NAME_GOODS_CATEGORY'] == 'Homewares':\n",
    "        return '3' \n",
    "    if row['NAME_GOODS_CATEGORY'] == 'Gardening':\n",
    "        return '3'\n",
    "    if row['NAME_GOODS_CATEGORY'] == 'Direct Sales':\n",
    "        return '3'     \n",
    "    if row['NAME_GOODS_CATEGORY'] == 'Vehicles':\n",
    "        return '4'   \n",
    "    if row['NAME_GOODS_CATEGORY'] == 'House Construction':\n",
    "        return '4'      \n",
    "    else:\n",
    "        return '0'\n",
    "    \n",
    "    \n",
    "def IsWeekend (row):\n",
    "    if row['WEEKDAY_APPR_PROCESS_START'] == 'SATURDAY':\n",
    "        return '1' \n",
    "    if row['WEEKDAY_APPR_PROCESS_START'] == 'SANDAY':\n",
    "        return '1'\n",
    "    else:\n",
    "        return '0'\n",
    "    \n",
    "    \n",
    "prev['IsWeekend'] = prev.apply(IsWeekend, axis=1)\n",
    "prev['GOOD_CAT'] = prev.apply(GOOD_CAT, axis=1)\n",
    "\n",
    "del prev['NAME_GOODS_CATEGORY']\n",
    "del prev['WEEKDAY_APPR_PROCESS_START']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prev, cat_cols = one_hot_encoder(prev)\n",
    "\n",
    "    # Days 365.243 values -> nan\n",
    "#prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "#prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "\n",
    "    # Add feature: value ask / value received percentage\n",
    "prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "prev['APP_ANNUTY_TO_CRED'] = prev['AMT_ANNUITY'] / prev['AMT_CREDIT']\n",
    "\n",
    "\n",
    "    # Previous applications numeric features\n",
    "num_aggregations = {\n",
    "        'AMT_ANNUITY': ['mean', 'sum', 'min', 'max', 'median', 'std', 'size'],\n",
    "        'AMT_CREDIT': ['mean', 'sum', 'min', 'max',  'median', 'std', 'size'],\n",
    "        'APP_CREDIT_PERC': ['mean', 'sum', 'min', 'max',  'median', 'std', 'size'],\n",
    "        'RATE_DOWN_PAYMENT': ['mean', 'sum', 'min', 'max', 'median', 'std', 'size'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum', 'min', 'max',  'median',  'std', 'size'],\n",
    "        'AMT_DOWN_PAYMENT' : ['mean', 'sum', 'min', 'max', 'median',  'std', 'size'],\n",
    "        'RATE_INTEREST_PRIVILEGED' : ['mean', 'sum', 'min', 'max',  'median',  'std', 'size'],\n",
    "        'DAYS_TERMINATION' : ['mean', 'sum', 'min', 'max',  'median',  'std', 'size'],\n",
    "        'AMT_GOODS_PRICE' : ['mean', 'sum', 'min', 'max', 'var', 'median',  'std', 'size'],\n",
    "        'APP_ANNUTY_TO_CRED' : ['mean', 'sum', 'min', 'max', 'median', 'std', 'size']\n",
    "    }\n",
    "\n",
    "    # Previous applications categorical features\n",
    "cat_aggregations = {}\n",
    "for cat in cat_cols:\n",
    "    cat_aggregations[cat] = ['count']\n",
    "\n",
    "prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations}).reset_index()\n",
    "prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "prev_agg['SK_ID_CURR']= prev_agg['PREV_SK_ID_CURR_']\n",
    "\n",
    "\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations).reset_index()\n",
    "approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations).reset_index()\n",
    "refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "\n",
    "refused_agg['SK_ID_CURR']= refused_agg['REFUSED_SK_ID_CURR_']\n",
    "approved_agg['SK_ID_CURR']= approved_agg['APPROVED_SK_ID_CURR_']\n",
    "prev = prev_agg.merge(refused_agg, how='left', on='SK_ID_CURR')\n",
    "prev = prev_agg.merge(approved_agg, how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev=prev.fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_csv('application_train.csv')\n",
    "target = t[['SK_ID_CURR', 'TARGET']]\n",
    "prev =prev.merge(target, how='left', on='SK_ID_CURR')\n",
    "new=prev[prev['TARGET'].notnull()]\n",
    "new=new.fillna(0)\n",
    "del new['PREV_SK_ID_CURR_']\n",
    "del new['APPROVED_SK_ID_CURR_']\n",
    "del new['PREV_AMT_ANNUITY_MEAN']\n",
    "new = new.replace([np.nan, -np.nan], 0)\n",
    "new=new.replace([np.inf, -np.inf], 0)\n",
    "new=abs(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выявяет самые низкие корреляции\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "select = SelectKBest(chi2, k=20)\n",
    "select.fit(new, new['TARGET'])\n",
    "# Get idxs of columns to keep\n",
    "mask = select.get_support()\n",
    "new_features = new.columns[mask]\n",
    "# Create new dataframe with only desired columns, or overwrite existing\n",
    "features_dataframe_new = new[new_features]\n",
    "#features_dataframe_new.corr()\n",
    "neew=pd.DataFrame(prev[new_features],prev['SK_ID_CURR']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "neew.to_csv('pprev.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "del prev['PREV_AMT_CREDIT_MEDIAN']\n",
    "del prev['PREV_CNT_PAYMENT_MEAN']\n",
    "del prev['PREV_CNT_PAYMENT_SUM']\n",
    "del prev['PREV_NAME_CONTRACT_TYPE_Consumer loans_COUNT']\n",
    "del prev['PREV_NAME_CONTRACT_TYPE_Revolving loans_COUNT']\n",
    "del prev['PREV_NAME_CONTRACT_TYPE_XNA_COUNT']\n",
    "del prev['PREV_FLAG_LAST_APPL_PER_CONTRACT_N_COUNT']\n",
    "del prev['PREV_FLAG_LAST_APPL_PER_CONTRACT_Y_COUNT']\n",
    "prev=prev.fillna(0)\n",
    "del prev['PREV_SK_ID_CURR_']\n",
    "del prev['APPROVED_SK_ID_CURR_']\n",
    "del prev['PREV_AMT_ANNUITY_MEAN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Выявляет самую сильную корреляцию\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "# Create and fit selector\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "selector.fit(prev, prev['PREV_NAME_CONTRACT_STATUS_Approved_COUNT'])\n",
    "# Get idxs of columns to keep\n",
    "mask = selector.get_support()\n",
    "new_features = prev.columns[mask]\n",
    "# Create new dataframe with only desired columns, or overwrite existing\n",
    "features_dataframe_new = prev[new_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выявяет самые низкие корреляции\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "select = SelectKBest(chi2, k=5)\n",
    "select.fit(prev, prev['PREV_NAME_CONTRACT_STATUS_Approved_COUNT'])\n",
    "# Get idxs of columns to keep\n",
    "mask = select.get_support()\n",
    "new_features = prev.columns[mask]\n",
    "# Create new dataframe with only desired columns, or overwrite existing\n",
    "features_dataframe_new = prev[new_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "features_dataframe_new.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "CCB = pd.read_csv('credit_card_balance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m(DPD):\n",
    "    \n",
    "    # DPD is a series of values of SK_DPD for each of the groupby combination \n",
    "    # We convert it to a list to get the number of SK_DPD values NOT EQUALS ZERO\n",
    "    x = DPD.tolist()\n",
    "    c = 0\n",
    "    for i,j in enumerate(x):\n",
    "        if j != 0:\n",
    "            c += 1\n",
    "    \n",
    "    return c \n",
    "\n",
    "#% of MINIMUM PAYMENTS MISSED\n",
    "\n",
    "def f(min_pay, total_pay):\n",
    "    \n",
    "    M = min_pay.tolist()\n",
    "    T = total_pay.tolist()\n",
    "    P = len(M)\n",
    "    c = 0 \n",
    "    # Find the count of transactions when Payment made is less than Minimum Payment \n",
    "    for i in range(len(M)):\n",
    "        if T[i] < M[i]:\n",
    "            c += 1  \n",
    "    return (100*c)/P\n",
    "\n",
    "\n",
    "#amt balance\n",
    "def amt(x1, x2):\n",
    "    \n",
    "    balance = x1.max()\n",
    "    limit = x2.max()\n",
    "    \n",
    "    return (balance/limit)\n",
    "\n",
    "#active credit\n",
    "def ac(x):\n",
    "    if x == 'Closed':\n",
    "        y = 0\n",
    "    else:\n",
    "        y = 1    \n",
    "    return y\n",
    "\n",
    "# days_cred\n",
    "def days_cred(x):\n",
    "    if x<0:\n",
    "        y = 0\n",
    "    else:\n",
    "        y = 1   \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# No of Loans per customer \n",
    "\n",
    "grp = CCB.groupby(by = ['SK_ID_CURR'])['SK_ID_PREV'].nunique().reset_index().rename(index = str, columns = {'SK_ID_PREV': 'NO_LOANS'})\n",
    "CCB = CCB.merge(grp, on = ['SK_ID_CURR'], how = 'left')\n",
    "\n",
    "# No of Installments paid per Loan per Customer \n",
    "\n",
    "grp = CCB.groupby(by = ['SK_ID_CURR', 'SK_ID_PREV'])['CNT_INSTALMENT_MATURE_CUM'].max().reset_index().rename(index = str, columns = {'CNT_INSTALMENT_MATURE_CUM': 'NO_INSTALMENTS'})\n",
    "grp1 = grp.groupby(by = ['SK_ID_CURR'])['NO_INSTALMENTS'].sum().reset_index().rename(index = str, columns = {'NO_INSTALMENTS': 'TOTAL_INSTALMENTS'})\n",
    "CCB = CCB.merge(grp1, on = ['SK_ID_CURR'], how = 'left')\n",
    "\n",
    "# Average Number of installments paid per loan \n",
    "\n",
    "CCB['INSTALLMENTS_PER_LOAN'] = (CCB['TOTAL_INSTALMENTS']/CCB['NO_LOANS']).astype('uint32')\n",
    "del CCB['TOTAL_INSTALMENTS']\n",
    "del CCB['NO_LOANS']\n",
    "\n",
    "CCB['AMT_CREDIT_LIMIT_ACTUAL1'] = CCB['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "\n",
    "# Calculate the ratio of Amount Balance to Credit Limit - CREDIT LOAD OF CUSTOMER \n",
    "# This is done for each Credit limit value per loan per Customer \n",
    "\n",
    "grp = CCB.groupby(by = ['SK_ID_CURR', 'SK_ID_PREV', 'AMT_CREDIT_LIMIT_ACTUAL']).apply(lambda x: amt(x.AMT_BALANCE, x.AMT_CREDIT_LIMIT_ACTUAL1)).reset_index().rename(index = str, columns = {0: 'CREDIT_LOAD1'})\n",
    "del CCB['AMT_CREDIT_LIMIT_ACTUAL1']\n",
    "\n",
    "# We now calculate the mean Credit load of All Loan transactions of Customer \n",
    "grp1 = grp.groupby(by = ['SK_ID_CURR'])['CREDIT_LOAD1'].mean().reset_index().rename(index = str, columns = {'CREDIT_LOAD1': 'CREDIT_LOAD'})\n",
    "CCB = CCB.merge(grp1, on = ['SK_ID_CURR'], how = 'left')\n",
    "del grp, grp1\n",
    "\n",
    "#AVERAGE NUMBER OF TIMES DAYS PAST DUE HAS OCCURRED PER CUSTOMER\n",
    "\n",
    "grp = CCB.groupby(by = ['SK_ID_CURR', 'SK_ID_PREV']).apply(lambda x: m(x.SK_DPD)).reset_index().rename(index = str, columns = {0: 'NO_DPD'})\n",
    "grp1 = grp.groupby(by = ['SK_ID_CURR'])['NO_DPD'].mean().reset_index().rename(index = str, columns = {'NO_DPD' : 'DPD_COUNT'})\n",
    "\n",
    "CCB = CCB.merge(grp1, on = ['SK_ID_CURR'], how = 'left')\n",
    "\n",
    "#AVERAGE OF DAYS PAST DUE PER CUSTOMER\n",
    "\n",
    "grp = CCB.groupby(by= ['SK_ID_CURR'])['SK_DPD'].mean().reset_index().rename(index = str, columns = {'SK_DPD': 'AVG_DPD'})\n",
    "CCB = CCB.merge(grp, on = ['SK_ID_CURR'], how = 'left')\n",
    "del grp \n",
    "\n",
    "\n",
    "grp = CCB.groupby(by = ['SK_ID_CURR']).apply(lambda x: f(x.AMT_INST_MIN_REGULARITY, x.AMT_PAYMENT_CURRENT)).reset_index().rename(index = str, columns = { 0 : 'PERCENTAGE_MISSED_PAYMENTS'})\n",
    "CCB = CCB.merge(grp, on = ['SK_ID_CURR'], how = 'left')\n",
    "\n",
    "# RATIO OF CASH VS CARD SWIPES\n",
    "\n",
    "grp = CCB.groupby(by = ['SK_ID_CURR'])['AMT_DRAWINGS_ATM_CURRENT'].sum().reset_index().rename(index = str, columns = {'AMT_DRAWINGS_ATM_CURRENT' : 'DRAWINGS_ATM'})\n",
    "CCB = CCB.merge(grp, on = ['SK_ID_CURR'], how = 'left')\n",
    "del grp\n",
    "gc.collect()\n",
    "\n",
    "grp = CCB.groupby(by = ['SK_ID_CURR'])['AMT_DRAWINGS_CURRENT'].sum().reset_index().rename(index = str, columns = {'AMT_DRAWINGS_CURRENT' : 'DRAWINGS_TOTAL'})\n",
    "CCB = CCB.merge(grp, on = ['SK_ID_CURR'], how = 'left')\n",
    "del grp\n",
    "gc.collect()\n",
    "\n",
    "CCB['CASH_CARD_RATIO1'] = (CCB['DRAWINGS_ATM']/CCB['DRAWINGS_TOTAL'])*100\n",
    "del CCB['DRAWINGS_ATM']\n",
    "del CCB['DRAWINGS_TOTAL']\n",
    "gc.collect()\n",
    "\n",
    "grp = CCB.groupby(by = ['SK_ID_CURR'])['CASH_CARD_RATIO1'].mean().reset_index().rename(index = str, columns ={ 'CASH_CARD_RATIO1' : 'CASH_CARD_RATIO'})\n",
    "CCB = CCB.merge(grp, on = ['SK_ID_CURR'], how = 'left')\n",
    "del grp \n",
    "gc.collect()\n",
    "\n",
    "del CCB['CASH_CARD_RATIO1']\n",
    "gc.collect()\n",
    "\n",
    "#AVERAGE DRAWING PER CUSTOMER\n",
    "\n",
    "grp = CCB.groupby(by = ['SK_ID_CURR'])['AMT_DRAWINGS_CURRENT'].sum().reset_index().rename(index = str, columns = {'AMT_DRAWINGS_CURRENT' : 'TOTAL_DRAWINGS'})\n",
    "CCB = CCB.merge(grp, on = ['SK_ID_CURR'], how = 'left')\n",
    "del grp\n",
    "gc.collect()\n",
    "\n",
    "grp = CCB.groupby(by = ['SK_ID_CURR'])['CNT_DRAWINGS_CURRENT'].sum().reset_index().rename(index = str, columns = {'CNT_DRAWINGS_CURRENT' : 'NO_DRAWINGS'})\n",
    "CCB = CCB.merge(grp, on = ['SK_ID_CURR'], how = 'left')\n",
    "del grp\n",
    "gc.collect()\n",
    "\n",
    "CCB['DRAWINGS_RATIO1'] = (CCB['TOTAL_DRAWINGS']/CCB['NO_DRAWINGS'])*100\n",
    "del CCB['TOTAL_DRAWINGS']\n",
    "del CCB['NO_DRAWINGS']\n",
    "gc.collect()\n",
    "\n",
    "grp = CCB.groupby(by = ['SK_ID_CURR'])['DRAWINGS_RATIO1'].mean().reset_index().rename(index = str, columns ={ 'DRAWINGS_RATIO1' : 'DRAWINGS_RATIO'})\n",
    "CCB = CCB.merge(grp, on = ['SK_ID_CURR'], how = 'left')\n",
    "del grp \n",
    "gc.collect()\n",
    "\n",
    "del CCB['DRAWINGS_RATIO1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "CCB.to_csv('CCB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau = pd.read_csv('bureau.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Loans per Customer\n",
    "NumLoanPerCust = bureau[['SK_ID_CURR', 'DAYS_CREDIT']].groupby(by = ['SK_ID_CURR'])['DAYS_CREDIT'].count().reset_index().rename(index=str, columns={'DAYS_CREDIT': 'BUREAU_LOAN_COUNT'})\n",
    "# Number of types of Credit loans for each Customer \n",
    "NumTypeofLoansPerCust = bureau[['SK_ID_CURR', 'CREDIT_TYPE']].groupby(by = ['SK_ID_CURR'])['CREDIT_TYPE'].nunique().reset_index().rename(index=str, columns={'CREDIT_TYPE': 'BUREAU_LOAN_TYPES'})\n",
    "\n",
    "bureau = bureau.merge(NumLoanPerCust, on = ['SK_ID_CURR'], how = 'left')\n",
    "bureau = bureau.merge(NumTypeofLoansPerCust, on = ['SK_ID_CURR'], how = 'left')\n",
    "\n",
    "# Average Number of Loans per Loan Type\n",
    "bureau['AVERAGE_LOAN_TYPE'] = bureau['BUREAU_LOAN_COUNT']/bureau['BUREAU_LOAN_TYPES']\n",
    "bureau['CREDIT_ACTIVE_BINARY'] = bureau['CREDIT_ACTIVE']\n",
    "bureau['CREDIT_ACTIVE_BINARY'] = bureau.apply(lambda x: ac(x.CREDIT_ACTIVE), axis = 1)\n",
    "\n",
    "# Calculate mean number of loans that are ACTIVE per CUSTOMER \n",
    "MeanNumofLoanActiveCust = bureau.groupby(by = ['SK_ID_CURR'])['CREDIT_ACTIVE_BINARY'].mean().reset_index().rename(index=str, columns={'CREDIT_ACTIVE_BINARY': 'ACTIVE_LOANS_PERCENTAGE'})\n",
    "\n",
    "# Groupby each Customer and Sort values of DAYS_CREDIT in ascending order\n",
    "DaysCredit = bureau[['SK_ID_CURR', 'SK_ID_BUREAU', 'DAYS_CREDIT']].groupby(by = ['SK_ID_CURR'])\n",
    "DaysCredit2 = DaysCredit.apply(lambda x: x.sort_values(['DAYS_CREDIT'], ascending = False)).reset_index(drop = True)\n",
    "\n",
    "\n",
    "# Calculate Difference between the number of Days \n",
    "DaysCredit2['DAYS_CREDIT1'] = DaysCredit2['DAYS_CREDIT']*-1\n",
    "DaysCredit2['DAYS_DIFF'] = DaysCredit2.groupby(by = ['SK_ID_CURR'])['DAYS_CREDIT1'].diff()\n",
    "DaysCredit2['DAYS_DIFF'] = DaysCredit2['DAYS_DIFF'].fillna(0).astype('uint32')\n",
    "\n",
    "bureau['CREDIT_ENDDATE_BINARY'] = bureau['DAYS_CREDIT_ENDDATE']\n",
    "bureau['CREDIT_ENDDATE_BINARY'] = bureau.apply(lambda x: days_cred(x.DAYS_CREDIT_ENDDATE), axis = 1)\n",
    "\n",
    "#% of LOANS PER CUSTOMER WHERE END DATE FOR CREDIT IS PAST\n",
    "LoansEndDate = bureau.groupby(by = ['SK_ID_CURR'])['CREDIT_ENDDATE_BINARY'].mean().reset_index().rename(index=str, columns={'CREDIT_ENDDATE_BINARY': 'CREDIT_ENDDATE_PERCENTAGE'})\n",
    "\n",
    "\n",
    "bureau['AMT_CREDIT_SUM_DEBT'] = bureau['AMT_CREDIT_SUM_DEBT'].fillna(0)\n",
    "bureau['AMT_CREDIT_SUM'] = bureau['AMT_CREDIT_SUM'].fillna(0)\n",
    "\n",
    "SumCredDebt = bureau[['SK_ID_CURR', 'AMT_CREDIT_SUM_DEBT']].groupby(by = ['SK_ID_CURR'])['AMT_CREDIT_SUM_DEBT'].sum().reset_index().rename( index = str, columns = { 'AMT_CREDIT_SUM_DEBT': 'TOTAL_CUSTOMER_DEBT'})\n",
    "SumCredDebt2 = bureau[['SK_ID_CURR', 'AMT_CREDIT_SUM']].groupby(by = ['SK_ID_CURR'])['AMT_CREDIT_SUM'].sum().reset_index().rename( index = str, columns = { 'AMT_CREDIT_SUM': 'TOTAL_CUSTOMER_CREDIT'})\n",
    "\n",
    "bureau = bureau.merge(SumCredDebt, on = ['SK_ID_CURR'], how = 'left')\n",
    "bureau = bureau.merge(SumCredDebt2, on = ['SK_ID_CURR'], how = 'left')\n",
    "\n",
    "bureau['DEBT_CREDIT_RATIO'] = bureau['TOTAL_CUSTOMER_DEBT']/bureau['TOTAL_CUSTOMER_CREDIT']\n",
    "bureau['AMT_CREDIT_SUM_DEBT'] = bureau['AMT_CREDIT_SUM_DEBT'].fillna(0)\n",
    "bureau['AMT_CREDIT_SUM_OVERDUE'] = bureau['AMT_CREDIT_SUM_OVERDUE'].fillna(0)\n",
    "\n",
    "AMT_CREDIT_SUM_DEBT = bureau[['SK_ID_CURR', 'AMT_CREDIT_SUM_DEBT']].groupby(by = ['SK_ID_CURR'])['AMT_CREDIT_SUM_DEBT'].sum().reset_index().rename( index = str, columns = { 'AMT_CREDIT_SUM_DEBT': 'TOTAL_CUSTOMER_DEBT'})\n",
    "AMT_CREDIT_SUM_OVERDUE = bureau[['SK_ID_CURR', 'AMT_CREDIT_SUM_OVERDUE']].groupby(by = ['SK_ID_CURR'])['AMT_CREDIT_SUM_OVERDUE'].sum().reset_index().rename( index = str, columns = { 'AMT_CREDIT_SUM_OVERDUE': 'TOTAL_CUSTOMER_OVERDUE'})\n",
    "\n",
    "bureau = bureau.merge(AMT_CREDIT_SUM_DEBT, on = ['SK_ID_CURR'], how = 'left')\n",
    "bureau = bureau.merge(AMT_CREDIT_SUM_OVERDUE, on = ['SK_ID_CURR'], how = 'left')\n",
    "bureau['OVERDUE_DEBT_RATIO'] = bureau['TOTAL_CUSTOMER_OVERDUE_y']/bureau['TOTAL_CUSTOMER_DEBT']\n",
    "\n",
    "CNT_CREDIT_PROLONG = bureau[['SK_ID_CURR', 'CNT_CREDIT_PROLONG']].groupby(by = ['SK_ID_CURR'])['CNT_CREDIT_PROLONG'].mean().reset_index().rename( index = str, columns = { 'CNT_CREDIT_PROLONG': 'AVG_CREDITDAYS_PROLONGED'})\n",
    "bureau = bureau.merge(CNT_CREDIT_PROLONG, on = ['SK_ID_CURR'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bureau['CREDIT_ENDDATE_BINARY'], bureau['BUREAU_LOAN_COUNT'], bureau['BUREAU_LOAN_TYPES'], \\\n",
    "bureau['CREDIT_ACTIVE_BINARY'] , bureau['DAYS_CREDIT_ENDDATE'],bureau['TOTAL_CUSTOMER_DEBT'], \\\n",
    "bureau['TOTAL_CUSTOMER_CREDIT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1265"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau.to_csv('bureau_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Column not found: DAYS_CREDIT_ENDDATE1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-1c365a694def>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Calculate the Difference in ENDDATES and fill missing values with zero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mDaysCredit2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DAYS_ENDDATE_DIFF'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDaysCredit2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'SK_ID_CURR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DAYS_CREDIT_ENDDATE1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mDaysCredit2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DAYS_ENDDATE_DIFF'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDaysCredit2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DAYS_ENDDATE_DIFF'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbureau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbureau\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDaysCredit2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'SK_ID_CURR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Column not found: {key}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gotitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Column not found: DAYS_CREDIT_ENDDATE1'"
     ]
    }
   ],
   "source": [
    "# Calculate the Difference in ENDDATES and fill missing values with zero \n",
    "DaysCredit2['DAYS_ENDDATE_DIFF'] = DaysCredit2.groupby(by = ['SK_ID_CURR'])['DAYS_CREDIT_ENDDATE1'].diff()\n",
    "DaysCredit2['DAYS_ENDDATE_DIFF'] = DaysCredit2['DAYS_ENDDATE_DIFF'].fillna(0).astype('uint32')\n",
    "\n",
    "bureau = bureau.merge(DaysCredit2, on = ['SK_ID_CURR'], how = 'left')\n",
    "\n",
    "AvgDaysEnddateDiff = bureau[['SK_ID_CURR', 'DAYS_ENDDATE_DIFF']].groupby(by = ['SK_ID_CURR'])['DAYS_ENDDATE_DIFF'].mean().reset_index().rename( index = str, columns = {'DAYS_ENDDATE_DIFF': 'AVG_ENDDATE_FUTURE'})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, cat_cols = one_hot_encoder(pos)\n",
    "# Features\n",
    "aggregations = {\n",
    "        'MONTHS_BALANCE': ['size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "for cat in cat_cols:\n",
    "    aggregations[cat] = ['mean']\n",
    "    \n",
    "pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "# Count pos cash accounts\n",
    "pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "pos=pos_agg.reset_index().copy()\n",
    "\n",
    "del pos['POS_NAME_CONTRACT_STATUS_Demand_MEAN']\n",
    "del pos['POS_NAME_CONTRACT_STATUS_Canceled_MEAN']\n",
    "del pos['POS_NAME_CONTRACT_STATUS_Approved_MEAN']\n",
    "del pos['POS_NAME_CONTRACT_STATUS_Amortized debt_MEAN']\n",
    "del pos['POS_NAME_CONTRACT_STATUS_XNA_MEAN']\n",
    "# del features with high correlation\n",
    "del pos['POS_SK_DPD_MEAN']\n",
    "del pos['POS_MONTHS_BALANCE_MEAN']\n",
    "del pos['POS_NAME_CONTRACT_STATUS_Completed_MEAN']\n",
    "del pos['POS_COUNT']\n",
    "del pos['POS_SK_DPD_MEAN']\n",
    "del pos['POS_SK_DPD_DEF_MEAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos.to_csv('new_pos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins, cat_cols = one_hot_encoder(ins)\n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "# Days past due and days before due (no negative values)\n",
    "ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "# Features: Perform aggregations\n",
    "aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max'],\n",
    "        'DBD': ['max', 'mean'],\n",
    "        'PAYMENT_PERC': ['var'],\n",
    "        'PAYMENT_DIFF': ['sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['sum']\n",
    "    }\n",
    "for cat in cat_cols:\n",
    "    aggregations[cat] = ['mean']\n",
    "ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "# Count installments accounts\n",
    "ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "ins=ins_agg.reset_index().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.to_csv('ins.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>INSTAL_NUM_INSTALMENT_VERSION_NUNIQUE</th>\n",
       "      <th>INSTAL_DPD_MAX</th>\n",
       "      <th>INSTAL_DBD_MAX</th>\n",
       "      <th>INSTAL_DBD_MEAN</th>\n",
       "      <th>INSTAL_PAYMENT_PERC_VAR</th>\n",
       "      <th>INSTAL_PAYMENT_DIFF_SUM</th>\n",
       "      <th>INSTAL_DAYS_ENTRY_PAYMENT_SUM</th>\n",
       "      <th>INSTAL_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>8.857143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15365.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100002</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>20.421053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5993.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100003</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.160000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-34633.0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100004</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2285.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100005</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>23.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5486.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  INSTAL_NUM_INSTALMENT_VERSION_NUNIQUE  INSTAL_DPD_MAX  \\\n",
       "0      100001                                      2            11.0   \n",
       "1      100002                                      2             0.0   \n",
       "2      100003                                      2             0.0   \n",
       "3      100004                                      2             0.0   \n",
       "4      100005                                      2             1.0   \n",
       "\n",
       "   INSTAL_DBD_MAX  INSTAL_DBD_MEAN  INSTAL_PAYMENT_PERC_VAR  \\\n",
       "0            36.0         8.857143                      0.0   \n",
       "1            31.0        20.421053                      0.0   \n",
       "2            14.0         7.160000                      0.0   \n",
       "3            11.0         7.666667                      0.0   \n",
       "4            37.0        23.666667                      0.0   \n",
       "\n",
       "   INSTAL_PAYMENT_DIFF_SUM  INSTAL_DAYS_ENTRY_PAYMENT_SUM  INSTAL_COUNT  \n",
       "0                      0.0                       -15365.0             7  \n",
       "1                      0.0                        -5993.0            19  \n",
       "2                      0.0                       -34633.0            25  \n",
       "3                      0.0                        -2285.0             3  \n",
       "4                      0.0                        -5486.0             9  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ins['INSTAL_COUNT'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train123.csv')\n",
    "prev=pd.read_csv('pprev.csv')\n",
    "cc=pd.read_csv('CCB.csv')\n",
    "bureau_new=pd.read_csv('bureau_new.csv')\n",
    "pos=pd.read_csv('new_pos.csv')\n",
    "ins=pd.read_csv('ins.csv')\n",
    "\n",
    "df = df.merge(prev, how='left', on='SK_ID_CURR')\n",
    "df = df.merge(pos, how='left', on='SK_ID_CURR')\n",
    "df = df.merge(cc, how='left', on='SK_ID_CURR')\n",
    "df = df.merge(ins, how='left', on='SK_ID_CURR')\n",
    "df = df.merge(bureau, how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(prev, how='left', on='SK_ID_CURR')\n",
    "df = df.merge(pos, how='left', on='SK_ID_CURR')\n",
    "df = df.merge(cc, how='left', on='SK_ID_CURR')\n",
    "df = df.merge(ins, how='left', on='SK_ID_CURR')\n",
    "df = df.merge(bureau, how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('new_full_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train[train['TARGET'].isnull()]\n",
    "train1= train[train['TARGET'] ==  1]\n",
    "train2= train[train['TARGET'] ==  0]\n",
    "train=pd.concat([train1, train2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feat = list(train.dtypes[train.dtypes == object].index)\n",
    "num_feat = [f for f in train if f not in cat_feat ]\n",
    "\n",
    "def get_woe_v1(df_train, df_test, col, target_col):\n",
    "    all_good = len(df_train[df_train[target_col] == 1][col])\n",
    "    all_bad = len(df_train[df_train[target_col] == 0][col])\n",
    "    odds_series = (\n",
    "        df_train[df_train[target_col] == 1][col].value_counts()\n",
    "        /\n",
    "        df_train[df_train[target_col] == 0][col].value_counts()\n",
    "    )\n",
    "    odds_series = odds_series / all_good * all_bad\n",
    "    category_woe_dict = np.log(odds_series).to_dict()\n",
    "    df_train[col + '_woe'] = df_train[col].apply(category_woe_dict.get)\n",
    "    df_test[col + '_woe'] = df_test[col].apply(category_woe_dict.get)\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "for cat_feat in cat_feat:\n",
    "    data_train, data_test = get_woe_v1(train, test, cat_feat, 'TARGET')\n",
    "    \n",
    "cat_feat = list(data_train.dtypes[data_train.dtypes == object].index)\n",
    "num_feat = [f for f in train if f not in cat_feat ]\n",
    "\n",
    "data_train=data_train.drop(cat_feat, axis=1)\n",
    "data_test=data_test.drop(cat_feat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imp=Imputer(missing_values='NaN', strategy='median', axis=1) \n",
    "imp.fit(data_train)\n",
    "data_train = imp.fit_transform(data_train)\n",
    "data_test = imp.fit_transform(data_test)\n",
    "\n",
    "#data_test = data_test.replace([np.nan, -np.nan], 0)\n",
    "#data_train = data_train.replace([np.nan, -np.nan], 0)\n",
    "#data_train=data_train.replace([np.inf, -np.inf], 0)\n",
    "#data_test=data_test.replace([np.inf, -np.inf], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "m=scaler.fit(data_train)\n",
    "X_train=scaler.transform(data_train)\n",
    "n=scaler.fit(data_test)\n",
    "X_test=scaler.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5, min_samples_leaf=20, max_features=0.8),\n",
    "                             n_estimators=20, learning_rate=0.1)\n",
    "clf_ada.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf_ada.predict_proba(X_test)\n",
    "sample_submission=pd.read_csv('sample_submission-9.csv')\n",
    "sample_submission['TARGET']=y_pred\n",
    "sample_submission.to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cc, cat_cols = one_hot_encoder(cc)\n",
    "# General aggregations\n",
    "cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "cc_agg = cc.groupby('SK_ID_CURR').agg(['mean', 'count'])\n",
    "cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "# Count credit card lines\n",
    "cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "cc=cc_agg.reset_index().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bb, bb_cat = one_hot_encoder(bb)\n",
    "bureau, bureau_cat = one_hot_encoder(bureau)\n",
    "    \n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "for col in bb_cat:\n",
    "    bb_aggregations[col] = ['mean']\n",
    "bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "#bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "\n",
    "num_aggregations = {\n",
    "        'DAYS_CREDIT': ['mean'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['max'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_ANNUITY': ['mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['sum']\n",
    "    }\n",
    "    # Bureau and bureau_balance categorical features\n",
    "cat_aggregations = {}\n",
    "for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    \n",
    "active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "\n",
    "# Bureau: Closed credits - using only numerical aggregations\n",
    "closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "\n",
    "bureau_agg=bureau_agg.reset_index()\n",
    "active_agg=active_agg.reset_index()\n",
    "closed_agg=closed_agg.reset_index()\n",
    "bureau_agg = bureau_agg.merge(closed_agg, how='left', on='SK_ID_CURR')\n",
    "bureau = bureau_agg.merge(active_agg, how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_df = df[df['TARGET'].notnull()]\n",
    "test_df = df[df['TARGET'].isnull()]\n",
    "print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "\n",
    "if stratified:\n",
    "    folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "else:\n",
    "    folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "# Create arrays and dataframes to store results\n",
    "oof_preds = np.zeros(train_df.shape[0])\n",
    "sub_preds = np.zeros(test_df.shape[0])\n",
    "feature_importance_df = pd.DataFrame()\n",
    "feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "    train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "    valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "    \n",
    "    clf = LGBMClassifier(\n",
    "            nthread=4,\n",
    "            n_estimators=10000,\n",
    "            learning_rate=0.02,\n",
    "            num_leaves=34,\n",
    "            colsample_bytree=0.9497036,\n",
    "            subsample=0.8715623,\n",
    "            max_depth=8,\n",
    "            reg_alpha=0.041545473,\n",
    "            reg_lambda=0.0735294,\n",
    "            min_split_gain=0.0222415,\n",
    "            min_child_weight=39.3259775,\n",
    "            silent=-1,\n",
    "            verbose=-1 )\n",
    "\n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "            eval_metric= 'auc', verbose= 100, early_stopping_rounds= 200)\n",
    "\n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n",
    "    # Write submission file and plot feature importance\n",
    "    if not debug:\n",
    "        test_df['TARGET'] = sub_preds\n",
    "        test_df[['SK_ID_CURR', 'TARGET']].to_csv(submission_file_name, index= False)\n",
    "    display_importances(feature_importance_df)\n",
    "    return feature_importance_df\n",
    "\n",
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances01.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
