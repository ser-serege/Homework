{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.5\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "views_programmes_schema = StructType(fields=[\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"ts_start\", IntegerType()),\n",
    "    StructField(\"ts_end\", IntegerType()),\n",
    "    StructField(\"item_type\", StringType())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 972 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfProgrammes = spark.read\\\n",
    "        .format(\"csv\")\\\n",
    "        .load(\"/labs/laba03/lab10_views_programmes.csv\", header='true', schema = views_programmes_schema)\\\n",
    "        .coalesce(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=0, item_id=7101053, ts_start=1491409931, ts_end=1491411600, item_type='live'),\n",
       " Row(user_id=0, item_id=7101054, ts_start=1491412481, ts_end=1491451571, item_type='live'),\n",
       " Row(user_id=0, item_id=7101054, ts_start=1491411640, ts_end=1491412481, item_type='live'),\n",
       " Row(user_id=0, item_id=6184414, ts_start=1486191290, ts_end=1486191640, item_type='live'),\n",
       " Row(user_id=257, item_id=4436877, ts_start=1490628499, ts_end=1490630256, item_type='live'),\n",
       " Row(user_id=1654, item_id=7489015, ts_start=1493434801, ts_end=1493435401, item_type='live'),\n",
       " Row(user_id=1654, item_id=7489023, ts_start=1493444101, ts_end=1493445601, item_type='live'),\n",
       " Row(user_id=1654, item_id=6617053, ts_start=1489186156, ts_end=1489200834, item_type='live'),\n",
       " Row(user_id=1654, item_id=6438693, ts_start=1487840070, ts_end=1487840433, item_type='live'),\n",
       " Row(user_id=1654, item_id=6526859, ts_start=1488705452, ts_end=1488706154, item_type='live'),\n",
       " Row(user_id=1654, item_id=6526754, ts_start=1488532396, ts_end=1488532895, item_type='pvr'),\n",
       " Row(user_id=1654, item_id=6239098, ts_start=1486732011, ts_end=1486732410, item_type='live')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfProgrammes.take(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, IntegerType\n",
    "views_train_schema = StructType(fields=[\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"purchase\", IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 24.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfTrain = spark.read\\\n",
    "        .format(\"csv\")\\\n",
    "        .load(\"/labs/laba03/lab10_train.csv\", header='true', schema=views_train_schema)\\\n",
    "        .coalesce(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  74107|       0|\n",
      "|   1654|  89249|       0|\n",
      "|   1654|  99982|       0|\n",
      "|   1654|  89901|       0|\n",
      "|   1654| 100504|       0|\n",
      "+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|   9897|       1|\n",
      "|   1654|   7394|       1|\n",
      "|   1654|   9064|       1|\n",
      "|   1654|  73216|       1|\n",
      "|   1654|  88816|       1|\n",
      "+-------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTrain.filter((dfTrain.user_id == 1654) & (dfTrain.purchase == 0)).show(5)\n",
    "dfTrain.filter((dfTrain.user_id == 1654) & (dfTrain.purchase == 1)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 14.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfTest = spark.read\\\n",
    "        .format(\"csv\")\\\n",
    "        .load(\"/labs/laba03/lab10_test.csv\", header='true', schema=views_train_schema)\\\n",
    "        .coalesce(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  94814|    null|\n",
      "|   1654|  93629|    null|\n",
      "|   1654|   9980|    null|\n",
      "|   1654|  95099|    null|\n",
      "|   1654|  11265|    null|\n",
      "|   1654|  88896|    null|\n",
      "|   1654|  67740|    null|\n",
      "|   1654|  74271|    null|\n",
      "|   1654|  99871|    null|\n",
      "|   1654|  78570|    null|\n",
      "+-------+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTest.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 501 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfItems = spark.read\\\n",
    "          .format(\"csv\")\\\n",
    "          .load(\"/labs/laba03/lab10_items.csv\", header='true', sep='\\t', inferShema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- channel_id: string (nullable = true)\n",
      " |-- datetime_availability_start: string (nullable = true)\n",
      " |-- datetime_availability_stop: string (nullable = true)\n",
      " |-- datetime_show_start: string (nullable = true)\n",
      " |-- datetime_show_stop: string (nullable = true)\n",
      " |-- content_type: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- region_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfItems.printSchema() #???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(item_id='65667', channel_id=None, datetime_availability_start='1970-01-01T00:00:00Z', datetime_availability_stop='2018-01-01T00:00:00Z', datetime_show_start=None, datetime_show_stop=None, content_type='1', title='на пробах только девушки (all girl auditions)', year='2013.0', genres='Эротика', region_id=None),\n",
       " Row(item_id='65669', channel_id=None, datetime_availability_start='1970-01-01T00:00:00Z', datetime_availability_stop='2018-01-01T00:00:00Z', datetime_show_start=None, datetime_show_stop=None, content_type='1', title='скуби ду: эротическая пародия (scooby doo: a xxx parody)', year='2011.0', genres='Эротика', region_id=None),\n",
       " Row(item_id='65668', channel_id=None, datetime_availability_start='1970-01-01T00:00:00Z', datetime_availability_stop='2018-01-01T00:00:00Z', datetime_show_start=None, datetime_show_stop=None, content_type='1', title='горячие девочки для горячих девочек (hot babes 4 hot babes)', year='2011.0', genres='Эротика', region_id=None)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfItems.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genre_unpivot = dfTrain.join(dfItems, on=\"item_id\")\\\n",
    "    .select(dfTrain.item_id, dfTrain.user_id, dfTrain.purchase, f.split(dfItems.genres,','))\\\n",
    "    .withColumn(\"genre\",f.explode(\"split(genres, ,)\"))\\\n",
    "    .drop(\"split(genres, ,)\")\\\n",
    "    .coalesce(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+--------------------+\n",
      "|item_id|user_id|purchase|               genre|\n",
      "+-------+-------+--------+--------------------+\n",
      "|  74107|   1654|       0|               Драмы|\n",
      "|  74107|   1654|       0|          Зарубежные|\n",
      "|  89249|   1654|       0|    Короткометражные|\n",
      "|  89249|   1654|       0|          Зарубежные|\n",
      "|  99982|   1654|       0|        Про животных|\n",
      "|  99982|   1654|       0|Западные мультфильмы|\n",
      "|  99982|   1654|       0|      Для всей семьи|\n",
      "|  99982|   1654|       0|           Для детей|\n",
      "|  99982|   1654|       0|          Зарубежные|\n",
      "|  89901|   1654|       0|               Драмы|\n",
      "+-------+-------+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_genre_unpivot.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genre_unpivot_test = dfTest.join(dfItems, on=\"item_id\")\\\n",
    "    .select(dfTest.item_id, dfTest.user_id, dfTest.purchase, f.split(dfItems.genres,','))\\\n",
    "    .withColumn(\"genre\",f.explode(\"split(genres, ,)\"))\\\n",
    "    .drop(\"split(genres, ,)\")\\\n",
    "    .coalesce(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genre_purchase =  df_genre_unpivot.filter(\"purchase == 1\").coalesce(3).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-------------------+\n",
      "|item_id|user_id|purchase|              genre|\n",
      "+-------+-------+--------+-------------------+\n",
      "|   7394|   1654|       1|        Мультфильмы|\n",
      "|   7394|   1654|       1|     Союзмультфильм|\n",
      "|   7394|   1654|       1|               Наши|\n",
      "|  73216|   1654|       1|               Наши|\n",
      "|  73216|   1654|       1|Для самых маленьких|\n",
      "|  88816|   1654|       1|        Приключения|\n",
      "|  88816|   1654|       1|     Документальные|\n",
      "|  88816|   1654|       1|         Зарубежные|\n",
      "|  72067| 510087|       1|          Детективы|\n",
      "|  72067| 510087|       1|              Драмы|\n",
      "+-------+-------+--------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_genre_purchase.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genre_train = df_genre_unpivot.alias(\"t1\").join(df_genre_purchase.alias(\"t2\"), on = f.expr(\"t1.user_id == t2.user_id and t1.genre == t2.genre\"))\\\n",
    "    .filter(f.expr(\"t1.item_id <> t2.item_id\"))\\\n",
    "    .groupby(\"t1.user_id\", \"t1.item_id\")\\\n",
    "    .sum(\"t2.purchase\")\\\n",
    "    .withColumnRenamed(\"sum(purchase)\",\"userGenrePurchase\")\\\n",
    "    .coalesce(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genre_test = df_genre_unpivot_test.alias(\"t1\").join(df_genre_purchase.alias(\"t2\"), on = f.expr(\"t1.user_id == t2.user_id and t1.genre == t2.genre\"))\\\n",
    "    .filter(f.expr(\"t1.item_id <> t2.item_id\"))\\\n",
    "    .groupby(\"t1.user_id\", \"t1.item_id\")\\\n",
    "    .sum(\"t2.purchase\")\\\n",
    "    .withColumnRenamed(\"sum(purchase)\",\"userGenrePurchase\")\\\n",
    "    .coalesce(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain = dfTrain\\\n",
    "    .withColumn(\"userPurchases\", f.sum(\"purchase\").over(Window.partitionBy(\"user_id\")) - col(\"purchase\"))\\\n",
    "    .withColumn(\"itemPopular\", f.sum(\"purchase\").over(Window.partitionBy(\"item_id\")) - col(\"purchase\"))\\\n",
    "    .join(dfProgrammes.filter(\"item_type == 'live'\").groupby(\"user_id\").count(), on=\"user_id\",how=\"left\")\\\n",
    "    .withColumn(\"live_watch_count\", col(\"count\") - col(\"purchase\"))\\\n",
    "    .drop(\"count\")\\\n",
    "    .join(dfProgrammes.filter(\"item_type == 'pvr'\").groupby(\"user_id\").count(), on=\"user_id\",how=\"left\")\\\n",
    "    .withColumn(\"pvr_watch_count\", col(\"count\") - col(\"purchase\"))\\\n",
    "    .drop(\"count\")\\\n",
    "    .join(dfItems.select(\"item_id\", col(\"year\").cast(\"double\")), on=\"item_id\",how=\"inner\")\\\n",
    "    .join(df_genre_train, on=[\"item_id\", \"user_id\"],how=\"left\")\\\n",
    "    .fillna(0)\\\n",
    "    .coalesce(3).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-------------+-----------+----------------+---------------+------+-----------------+\n",
      "|item_id|user_id|purchase|userPurchases|itemPopular|live_watch_count|pvr_watch_count|  year|userGenrePurchase|\n",
      "+-------+-------+--------+-------------+-----------+----------------+---------------+------+-----------------+\n",
      "|    326| 659698|       0|            1|          1|             126|            141|2012.0|                0|\n",
      "|    326| 747141|       0|            1|          1|              87|            273|2012.0|                3|\n",
      "|    326| 840492|       0|            0|          1|             222|            183|2012.0|                0|\n",
      "|    326| 891603|       0|            5|          1|             266|            299|2012.0|                0|\n",
      "|    326| 903828|       0|            1|          1|               1|              1|2012.0|                0|\n",
      "|    326| 921813|       0|            1|          1|              23|              9|2012.0|                0|\n",
      "|    326| 933055|       0|            0|          1|              46|              0|2012.0|                0|\n",
      "|    336| 847580|       0|            9|          0|              46|              1|2012.0|                1|\n",
      "|    336| 857884|       0|            2|          0|              77|             25|2012.0|                0|\n",
      "|    336| 890147|       0|            1|          0|             335|            579|2012.0|                1|\n",
      "+-------+-------+--------+-------------+-----------+----------------+---------------+------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTrain.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest = dfTest\\\n",
    "    .join(dfTrain.groupby(\"user_id\").sum(\"purchase\"), on=\"user_id\",how=\"left\")\\\n",
    "    .withColumnRenamed(\"sum(purchase)\", \"userPurchases\")\\\n",
    "    .join(dfTrain.groupby(\"item_id\").sum(\"purchase\"), on=\"item_id\",how=\"left\")\\\n",
    "    .withColumnRenamed(\"sum(purchase)\", \"itemPopular\")\\\n",
    "    .join(dfProgrammes.filter(\"item_type == 'live'\").groupby(\"user_id\").count(), on=\"user_id\",how=\"left\")\\\n",
    "    .withColumnRenamed(\"count\", \"live_watch_count\")\\\n",
    "    .join(dfProgrammes.filter(\"item_type == 'pvr'\").groupby(\"user_id\").count(), on=\"user_id\",how=\"left\")\\\n",
    "    .withColumnRenamed(\"count\", \"pvr_watch_count\")\\\n",
    "    .join(dfItems.select(\"item_id\", col(\"year\").cast(\"double\")), on=\"item_id\",how=\"inner\")\\\n",
    "    .join(df_genre_test, on=[\"item_id\", \"user_id\"],how=\"left\")\\\n",
    "    .fillna(0)\\\n",
    "    .coalesce(3).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-------------+-----------+----------------+---------------+------+-----------------+\n",
      "|item_id|user_id|purchase|userPurchases|itemPopular|live_watch_count|pvr_watch_count|  year|userGenrePurchase|\n",
      "+-------+-------+--------+-------------+-----------+----------------+---------------+------+-----------------+\n",
      "|    326| 762457|       0|           69|          1|             513|             25|2012.0|               29|\n",
      "|    326| 878144|       0|            1|          1|             386|              5|2012.0|                0|\n",
      "|    326| 904529|       0|            0|          1|             110|             14|2012.0|                0|\n",
      "|    326| 923368|       0|            0|          1|              55|              0|2012.0|                0|\n",
      "|    336| 894930|       0|            1|          0|             283|             44|2012.0|                1|\n",
      "|    357| 885902|       0|            2|          1|             371|             59|2012.0|                0|\n",
      "|    357| 923825|       0|           17|          1|             363|             10|2012.0|               13|\n",
      "|    396| 780804|       0|            4|          1|             477|              1|2007.0|                5|\n",
      "|    396| 820048|       0|            1|          1|             193|              3|2007.0|                2|\n",
      "|    396| 833749|       0|            1|          1|             173|             13|2007.0|                0|\n",
      "+-------+-------+--------+-------------+-----------+----------------+---------------+------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTest.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assemblerInputs = ['userPurchases', 'itemPopular', 'live_watch_count', 'pvr_watch_count', 'year', 'userGenrePurchase']\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "df_features = assembler.transform(dfTrain).coalesce(3)\n",
    "df_features_test = assembler.transform(dfTest).coalesce(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_features.sampleBy(\"purchase\", fractions={0: 0.75, 1: 0.75}).coalesce(3).cache()\n",
    "test = df_features.join(train, (df_features.user_id == train.user_id) & (df_features.item_id == train.item_id)\\\n",
    "                        , how=\"leftanti\").coalesce(3).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-------------+-----------+----------------+---------------+------+-----------------+--------------------+\n",
      "|item_id|user_id|purchase|userPurchases|itemPopular|live_watch_count|pvr_watch_count|  year|userGenrePurchase|            features|\n",
      "+-------+-------+--------+-------------+-----------+----------------+---------------+------+-----------------+--------------------+\n",
      "|    326| 659698|       0|            1|          1|             126|            141|2012.0|                0|[1.0,1.0,126.0,14...|\n",
      "|    326| 747141|       0|            1|          1|              87|            273|2012.0|                3|[1.0,1.0,87.0,273...|\n",
      "|    326| 840492|       0|            0|          1|             222|            183|2012.0|                0|[0.0,1.0,222.0,18...|\n",
      "|    326| 933055|       0|            0|          1|              46|              0|2012.0|                0|[0.0,1.0,46.0,0.0...|\n",
      "|    336| 847580|       0|            9|          0|              46|              1|2012.0|                1|[9.0,0.0,46.0,1.0...|\n",
      "+-------+-------+--------+-------------+-----------+----------------+---------------+------+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+-------+--------+-------------+-----------+----------------+---------------+------+-----------------+--------------------+\n",
      "|item_id|user_id|purchase|userPurchases|itemPopular|live_watch_count|pvr_watch_count|  year|userGenrePurchase|            features|\n",
      "+-------+-------+--------+-------------+-----------+----------------+---------------+------+-----------------+--------------------+\n",
      "|  67318|   1654|       0|            5|          1|             125|             73|2014.0|                1|[5.0,1.0,125.0,73...|\n",
      "|  73409|   1654|       0|            5|          0|             125|             73|2011.0|                0|[5.0,0.0,125.0,73...|\n",
      "|  77673|   1654|       0|            5|          2|             125|             73|2010.0|                1|[5.0,2.0,125.0,73...|\n",
      "|  95599|   1654|       0|            5|          1|             125|             73|1955.0|                0|[5.0,1.0,125.0,73...|\n",
      "|  79856| 510087|       0|            6|          6|             441|            131|2012.0|                3|[6.0,6.0,441.0,13...|\n",
      "+-------+-------+--------+-------------+-----------+----------------+---------------+------+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)\n",
    "test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='purchase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 ms, sys: 4 ms, total: 32 ms\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(featuresCol = 'features', labelCol = 'purchase', maxDepth=8, maxIter=15, seed = 42)\n",
    "model = gbt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+--------------------+--------------------+----------+\n",
      "|user_id|item_id|purchase|       rawPrediction|         probability|prediction|\n",
      "+-------+-------+--------+--------------------+--------------------+----------+\n",
      "|   1654|  67318|       0|[1.44554720307470...|[0.94740444172881...|       0.0|\n",
      "|   1654|  73409|       0|[1.44521513863541...|[0.94737133883980...|       0.0|\n",
      "|   1654|  77673|       0|[1.44554720307470...|[0.94740444172881...|       0.0|\n",
      "|   1654|  95599|       0|[1.44554720307470...|[0.94740444172881...|       0.0|\n",
      "| 510087|  79856|       0|[1.43340835550160...|[0.94618147831327...|       0.0|\n",
      "| 510087|  92538|       0|[1.42814080026813...|[0.94564247987134...|       0.0|\n",
      "| 510087|  93562|       0|[1.44244594234322...|[0.94709451557742...|       0.0|\n",
      "| 510087|  96398|       0|[1.44439912856999...|[0.94728990869985...|       0.0|\n",
      "| 510087|  97941|       0|[1.44510650417007...|[0.94736050500012...|       0.0|\n",
      "| 517612|  72029|       0|[1.44565033526848...|[0.94741471878331...|       0.0|\n",
      "+-------+-------+--------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------+-------+--------+--------------------+--------------------+----------+\n",
      "|user_id|item_id|purchase|       rawPrediction|         probability|prediction|\n",
      "+-------+-------+--------+--------------------+--------------------+----------+\n",
      "| 870461|  74570|       1|[-0.7021007946698...|[0.19715022908659...|       1.0|\n",
      "| 888153|  10320|       1|[-0.6713917656475...|[0.20705268052836...|       1.0|\n",
      "| 874657|  89643|       0|[-0.2350224689553...|[0.38460560746315...|       1.0|\n",
      "| 749587|  66773|       1|[-0.7986505553474...|[0.16835915995111...|       1.0|\n",
      "| 749587|  94534|       1|[-0.8132243948905...|[0.16431742236493...|       1.0|\n",
      "| 747028|  99806|       1|[-0.3964361392691...|[0.31155226849240...|       1.0|\n",
      "| 857875|   9705|       0|[-0.3431694136800...|[0.33484801360654...|       1.0|\n",
      "| 831698|   9705|       0|[-0.2354618246433...|[0.38439765152174...|       1.0|\n",
      "| 844427|  88963|       1|[-0.8297664538834...|[0.15982470848330...|       1.0|\n",
      "| 825692|   8628|       1|[-0.7425617248938...|[0.18465479939375...|       1.0|\n",
      "+-------+-------+--------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "predictions.select(\"user_id\", \"item_id\", \"purchase\", \"rawPrediction\", \"probability\", \"prediction\").show(10)\n",
    "predictions.filter(predictions.prediction == 1).select(\"user_id\", \"item_id\", \"purchase\", \"rawPrediction\", \"probability\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.935562156697861\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 ms, sys: 8 ms, total: 40 ms\n",
      "Wall time: 3min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(featuresCol = 'features', labelCol = 'purchase', maxDepth=8, maxIter=15, seed = 42)\n",
    "model = gbt.fit(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=717302, item_id=77442, rawPrediction=DenseVector([-0.2139, 0.2139]), probability=DenseVector([0.3947, 0.6053]), prediction=1.0),\n",
       " Row(user_id=588378, item_id=89624, rawPrediction=DenseVector([-1.0126, 1.0126]), probability=DenseVector([0.1166, 0.8834]), prediction=1.0),\n",
       " Row(user_id=588378, item_id=74390, rawPrediction=DenseVector([-0.3813, 0.3813]), probability=DenseVector([0.3181, 0.6819]), prediction=1.0),\n",
       " Row(user_id=833685, item_id=89624, rawPrediction=DenseVector([-0.3437, 0.3437]), probability=DenseVector([0.3346, 0.6654]), prediction=1.0),\n",
       " Row(user_id=878599, item_id=5510, rawPrediction=DenseVector([-0.4166, 0.4166]), probability=DenseVector([0.3029, 0.6971]), prediction=1.0),\n",
       " Row(user_id=834405, item_id=8658, rawPrediction=DenseVector([-0.1573, 0.1573]), probability=DenseVector([0.422, 0.578]), prediction=1.0),\n",
       " Row(user_id=851745, item_id=10467, rawPrediction=DenseVector([-0.134, 0.134]), probability=DenseVector([0.4334, 0.5666]), prediction=1.0),\n",
       " Row(user_id=782482, item_id=93666, rawPrediction=DenseVector([-0.7911, 0.7911]), probability=DenseVector([0.1705, 0.8295]), prediction=1.0),\n",
       " Row(user_id=747028, item_id=8661, rawPrediction=DenseVector([-1.0126, 1.0126]), probability=DenseVector([0.1166, 0.8834]), prediction=1.0),\n",
       " Row(user_id=588378, item_id=88963, rawPrediction=DenseVector([-0.4478, 0.4478]), probability=DenseVector([0.29, 0.71]), prediction=1.0)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.transform(df_features_test)\n",
    "predictions.select(\"user_id\", \"item_id\", \"rawPrediction\", \"probability\", \"prediction\").take(20)\n",
    "predictions.filter(predictions.prediction == 1).select(\"user_id\", \"item_id\", \"rawPrediction\", \"probability\", \"prediction\").take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as f\n",
    "secondElement=f.udf(lambda v:float(v[1]),FloatType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = predictions\\\n",
    "    .withColumn(\"purchase\", secondElement(\"probability\"))\\\n",
    "    .select(\"user_id\", \"item_id\", \"purchase\")\\\n",
    "    .orderBy(\"user_id\", \"item_id\")\\\n",
    "    .toPandas().to_csv('lab03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
