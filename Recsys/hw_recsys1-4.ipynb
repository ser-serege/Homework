{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pyspark.sql.functions as sql_func\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import numpy as np\n",
    "from pyspark.sql.types import FloatType, ArrayType\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tf_idf = spark.read.parquet(os.path.join(DATA_DIR, \"tf_idf.parquet\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#ranking counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ratings = (spark.read.csv(os.path.join(DATA_DIR, \"ratings.csv\"),header=True,inferSchema=True)\n",
    "    .select(\"movieId\", \"userId\", \"rating\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# averege rank by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "user_avg = ratings.groupBy('userId').agg(sql_func.avg(\"rating\").alias(\"avg_rating_user\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averege rank by movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_avg = ratings.groupBy('movieID').agg(sql_func.avg(\"rating\").alias(\"avg_rating_movie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_all_data = (ratings.alias(\"r\")\\\n",
    "        .join(tf_idf.alias(\"tf-idf\"), sql_func.col(\"tf-idf.movieId\") ==  sql_func.col(\"r.movieId\"))\\\n",
    "        .join(user_avg.alias(\"user\"), sql_func.col(\"user.userId\") ==  sql_func.col(\"r.userId\"))\\\n",
    "        .join(movie_avg.alias(\"movie\"), sql_func.col(\"movie.movieId\") ==  sql_func.col(\"r.movieId\"))\\\n",
    "        .select(sql_func.col(\"r.userId\"),sql_func.col(\"r.movieId\"),sql_func.col(\"r.rating\"),\\\n",
    "        sql_func.col(\"user.avg_rating_user\"),sql_func.col(\"movie.avg_rating_movie\"),sql_func.col(\"tf-idf.tf_idf\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def sklearn_lr(spark_x: list, spark_y: list) -> list:\n",
    "    numpy_x = np.array([vector.toArray() for vector in spark_x])\n",
    "    numpy_y = np.array(spark_y).reshape(-1, 1)\n",
    "    lr = ElasticNet().fit(numpy_x, numpy_y)\n",
    "    return [lr.sparse_coef_.todense().tolist()[0], lr.intercept_.tolist()]\n",
    "\n",
    "reg_udf = sql_func.udf( sklearn_lr, returnType=ArrayType(ArrayType(FloatType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train _ test _ split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data  = join_all_data.select(sql_func.col(\"userId\"),sql_func.col(\"movieId\"),sql_func.col(\"rating\"),\\\n",
    "        list_concat(\"tf_idf\",\"avg_rating_user\",\"avg_rating_movie\").alias(\"tf_idf\"))\\.randomSplit([0.8, 0.2], seed=42)\n",
    "                         \n",
    "train_data.cache()\n",
    "test_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model_coef = (train_data.groupBy(\"userId\").agg(\n",
    "        sql_func.collect_list(\"tf_idf\").alias(\"x\"),\n",
    "        sql_func.collect_list(\"rating\").alias(\"y\"))\n",
    "    .withColumn(\"model_coeff\", reg_udf(\"x\", \"y\")).cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def lr_apply(x: SparseVector, lr_coef: list) -> float:\n",
    "    return float(np.array(x).dot(np.array(lr_coef[0])) + lr_coef[1][0])\n",
    "\n",
    "\n",
    "lr_apply_udf = sql_func.udf(lr_apply, returnType=FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make prediction func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_prediction(data: DataFrame) -> DataFrame:\n",
    "    return (\n",
    "        data\n",
    "        .join(model_coef, \"userId\")\n",
    "        .select(\n",
    "            \"userId\",\n",
    "            \"rating\",\n",
    "            \"movieId\",\n",
    "            \"tf_idf\", \n",
    "            lr_apply_udf(\"tf_idf\", \"model_coeff\").alias(\"prediction\"))\n",
    "        .cache()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_prediction = get_prediction(train_data)\n",
    "(train_prediction.write.mode(\"overwrite\")\n",
    "    .parquet(os.path.join(DATA_DIR, \"train_prediction.parquet\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_prediction(prediction: DataFrame) -> float:\n",
    "    return np.sqrt(\n",
    "        prediction\n",
    "        .selectExpr(\"\"\"\n",
    "            CASE\n",
    "                WHEN prediction > 5 THEN 5\n",
    "                WHEN prediction < 0.5 THEN 0.5\n",
    "                ELSE prediction\n",
    "            END AS prediction\n",
    "        \"\"\", \"rating\")\n",
    "        .select(\n",
    "            sql_func.pow(sql_func.col(\"rating\") - sql_func.col(\"prediction\"), 2)\n",
    "            .alias(\"squared_error\")\n",
    "        )\n",
    "        .agg(sql_func.avg(\"squared_error\"))\n",
    "        .first()[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_prediction(train_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test_prediction = get_prediction(test_data)\n",
    "(\n",
    "test_prediction.write.mode(\"overwrite\")\n",
    "    .parquet(os.path.join(DATA_DIR, \"test_prediction.parquet\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_prediction(test_prediction)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
