{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Домашнее задание по NLP # 3 [100 баллов] \n",
    "## Классификация и генерация фамилий \n",
    "\n",
    "В этом домашнем задании вам предстоит классифицировать и генерировать фамилии на 19 разных языках. Обучающие данные хранятся в папке data и разбиты по языкам: один язык – одна подпапка. Ниже представлен код для считывания данных в словарь вида: \n",
    "```d{язык} : [список имен]```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, Dropout\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../../Data/NLP-HW3/names\\\\Arabic.txt', '../../../Data/NLP-HW3/names\\\\Chinese.txt', '../../../Data/NLP-HW3/names\\\\Czech.txt', '../../../Data/NLP-HW3/names\\\\Dutch.txt', '../../../Data/NLP-HW3/names\\\\English.txt', '../../../Data/NLP-HW3/names\\\\French.txt', '../../../Data/NLP-HW3/names\\\\German.txt', '../../../Data/NLP-HW3/names\\\\Greek.txt', '../../../Data/NLP-HW3/names\\\\Irish.txt', '../../../Data/NLP-HW3/names\\\\Italian.txt', '../../../Data/NLP-HW3/names\\\\Japanese.txt', '../../../Data/NLP-HW3/names\\\\Korean.txt', '../../../Data/NLP-HW3/names\\\\Polish.txt', '../../../Data/NLP-HW3/names\\\\Portuguese.txt', '../../../Data/NLP-HW3/names\\\\Russian.txt', '../../../Data/NLP-HW3/names\\\\Scottish.txt', '../../../Data/NLP-HW3/names\\\\Spanish.txt', '../../../Data/NLP-HW3/names\\\\Vietnamese.txt']\n",
      "['Abels', 'Abelsky', 'Abeltsev', 'Abelyan', 'Aberson', 'Abertasov', 'Abesadze', 'Abezgauz', 'Abgaryan', 'Abibulaev', 'Abidoff', 'Abidov', 'Abih', 'Abikh', 'Abisaloff', 'Abisalov', 'Abitoff', 'Abitov', 'Abjaliloff', 'Abjalilov']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('../../../Data/NLP-HW3/names/*.txt'))\n",
    "\n",
    "\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return lines\n",
    "\n",
    "all_categories = []\n",
    "category_lines = {}\n",
    "for filename in findFiles('../../../Data/NLP-HW3/names/*.txt'):\n",
    "    category = filename.split('/')[-1].split('.')[0][6:]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "print(category_lines['Russian'][100:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('names.csv')\n",
    "df.drop('Unnamed: 0', axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Предварительная обработка данных [10 баллов]\n",
    "\n",
    "1. Удалите неоднозначные фамилии (т.е. одинаковые фамилии на разных языка), если такие есть;\n",
    "2. Оцените \n",
    "* среднюю длину фамилии по всей коллекции\n",
    "* по каждому языку\n",
    "3. Для последующей классификации (части 2 и 3) оздайте обучающее и тестовое множество так, чтобы в обучающем множестве классы были сбалансированы: то есть, в обучающее множество должно входить примерно одинаковое количество фамилий на разных ящыка. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удалите неоднозначные фамилии (т.е. одинаковые фамилии на разных языка), если такие есть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20074, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17458, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оцените среднюю длину фамилии по всей коллекции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Len'] = df.Name.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.366422270592278"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Len.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### По каждому языку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lang</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Arabic</th>\n",
       "      <td>5.592593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <td>3.674797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Czech</th>\n",
       "      <td>6.582505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dutch</th>\n",
       "      <td>6.939929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>6.413517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>7.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>German</th>\n",
       "      <td>6.810680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Greek</th>\n",
       "      <td>8.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Irish</th>\n",
       "      <td>7.341463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italian</th>\n",
       "      <td>6.986957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japanese</th>\n",
       "      <td>6.704868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean</th>\n",
       "      <td>3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Polish</th>\n",
       "      <td>7.349206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portuguese</th>\n",
       "      <td>6.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russian</th>\n",
       "      <td>8.096309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scottish</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>6.714932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vietnamese</th>\n",
       "      <td>3.785714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Len\n",
       "Lang                \n",
       "Arabic      5.592593\n",
       "Chinese     3.674797\n",
       "Czech       6.582505\n",
       "Dutch       6.939929\n",
       "English     6.413517\n",
       "French      7.068182\n",
       "German      6.810680\n",
       "Greek       8.843750\n",
       "Irish       7.341463\n",
       "Italian     6.986957\n",
       "Japanese    6.704868\n",
       "Korean      3.500000\n",
       "Polish      7.349206\n",
       "Portuguese  6.500000\n",
       "Russian     8.096309\n",
       "Scottish    3.000000\n",
       "Spanish     6.714932\n",
       "Vietnamese  3.785714"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Lang').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Lang'] != 'Scottish']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стратифицированное разбиение выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df.Lang\n",
    "X = df.Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(s):\n",
    "    return(s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.apply(lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=777)\n",
    "\n",
    "for train_index, test_index in splitter.split(X, Y):\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    \n",
    "    Y_train = Y.iloc[train_index]\n",
    "    Y_test = Y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13965, 13965, 3492, 3492)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(Y_train), len(X_test), len(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Часть 2. Базовый метод классификации [20 баллов]\n",
    "\n",
    "\n",
    "\n",
    "Используйте метод наивного Байеса, логистическую регрессию или любой другой метод для классификации фамилий: в качестве признаков используйте символьные $n$-граммы. Сравните результаты, получаемые при разных $n=2,3,4$ по $F$-мере и аккуратности. В каких случаях метод ошибается?\n",
    "\n",
    "Для генерации $n$-грамм используйте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(s):\n",
    "    l = list(ngrams(s, 3))\n",
    "    ngr_str = ''\n",
    "    for i in range(len(l)):        \n",
    "        ngr = ''\n",
    "        for j in l[i]:\n",
    "            ngr += j\n",
    "        ngr_str += ngr\n",
    "        ngr_str += ' '\n",
    "    return(ngr_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(get_ngrams)\n",
    "X_test = X_test.apply(get_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([ \n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()), \n",
    "    ('clf', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = Y_test\n",
    "predictions = Y_pred\n",
    "print(\"Precision: {0:6.2f}\".format(precision_score(true, predictions, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(true, predictions, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(true, predictions, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(true, predictions)))\n",
    "print(classification_report(true, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Часть 3. Нейронная сеть [35 баллов]\n",
    "\n",
    "\n",
    "Используйте  реккурентную нейронную сеть с  LSTM для решения задачи. В ней может быть несколько слоев с LSTM, несколько слоев c Bidirectional(LSTM).  У нейронной сети один выход, определяющий класс фамилии. \n",
    "\n",
    "Представление имени для классификации в этом случае: \n",
    "1 вариант: бинарная матрица размера (количество букв в корпусе $\\times$ максимальная длина имени). Обозначим его через $x$. Если первая буква имени a, то $x[1][1] = 1$, если вторая – b, то  $x[2][1] = 1$. То есть, используем one hot encoding для векторизации букв.  \n",
    "2 вариант: Embedding'и символов. \n",
    "\n",
    "Выберите тот вариант, который вам проще или интереснее реализовать :) \n",
    "\n",
    "Не забудьте про регуляризацию нейронной сети дропаутами. \n",
    "\n",
    "Сравните результаты классификации разными методами по accuracy, micro- и macro- F-measure, precision, recall. Какой метод лучше и почему?\n",
    "\n",
    "Сравните результаты, получаемые при разных значениях дропаута, разных числах узлов на слоях нейронной сети по $F$-мере и аккуратности. В каких случаях нейронная сеть ошибается?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если совсем не получается запрограммировать нейронную сеть самостоятельно, обратитесь к туториалу тут: https://github.com/divamgupta/lstm-gender-predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lang</th>\n",
       "      <th>Name</th>\n",
       "      <th>Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>Khoury</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>Nahas</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>Daher</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>Gerges</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>Nazari</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Lang    Name  Len\n",
       "0  Arabic  Khoury    6\n",
       "1  Arabic   Nahas    5\n",
       "2  Arabic   Daher    5\n",
       "3  Arabic  Gerges    6\n",
       "4  Arabic  Nazari    6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = df.Len.max()\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17457,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = set(  \"\".join(X))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reset_index(drop=True)\n",
    "Y = Y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_len = Y.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((len(X) , max_len, len(chars) ), dtype=np.bool)\n",
    "y = np.zeros((len(X) , Y.nunique() ), dtype=np.bool)\n",
    "\n",
    "\n",
    "for i, name in enumerate(X):\n",
    "    for t, char in enumerate(name):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i][Y.unique().searchsorted(Y[i])] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, input_shape=(max_len, len(chars))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(out_len))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 20, 128)           95744     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 17)                2193      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 17)                0         \n",
      "=================================================================\n",
      "Total params: 229,521\n",
      "Trainable params: 229,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "                                                        x,\n",
    "                                                        y,\n",
    "                                                        shuffle=True,\n",
    "                                                        train_size=0.8,\n",
    "                                                        random_state=42\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "13965/13965 [==============================] - 74s 5ms/step - loss: 0.1105\n",
      "Epoch 2/4\n",
      "13965/13965 [==============================] - 72s 5ms/step - loss: 0.0943\n",
      "Epoch 3/4\n",
      "13965/13965 [==============================] - 73s 5ms/step - loss: 0.0864\n",
      "Epoch 4/4\n",
      "13965/13965 [==============================] - 79s 6ms/step - loss: 0.0809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xfb81da0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=16, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_preds)):\n",
    "    max_ = 0\n",
    "    for j in range(len(y_preds[i])):\n",
    "        if y_preds[i][j] > max_:\n",
    "            max_ = y_preds[i][j]\n",
    "    for j in range(len(y_preds[i])):\n",
    "        if y_preds[i][j] == max_:\n",
    "            y_preds[i][j] = True\n",
    "        else:\n",
    "            y_preds[i][j] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:   0.25\n",
      "Recall:   0.25\n",
      "F1-measure:   0.23\n",
      "Accuracy:   0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "true = y_valid\n",
    "predictions = y_preds\n",
    "print(\"Precision: {0:6.2f}\".format(precision_score(true, predictions, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(true, predictions, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(true, predictions, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(true, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Часть 4. Генерация фамилии [35 баллов]\n",
    "\n",
    "Используйте архитектуру нейронной сети из Части 3 для генерации имени. В этот момент можно забыть про разбиение коллекции на обучающее и тестовое множество. \n",
    "\n",
    "Рассмотрите два сценария генерация имени:\n",
    "* обучение нейронной сети по всей коллекции\n",
    "* обучение нейронной сети с обуславливанием на язык\n",
    "\n",
    "Для обуславливания на язык нужно изменить векторное представление фамилии. До этого момента мы разбивали фамилию на отдельные символы и находили векторное представление каждого символа. Теперь добавим в начало фамилии метку языка и будем ее считать первым символов фамилии. Так нейронная сеть научиться понимать, на каком языке написана фамилия. \n",
    "\n",
    "Пример: ```[rus bos i v a n o v eos pad pad pad]```\n",
    "\n",
    "Когда будем генерировать новую фамилию, будем начинать процесс генерации не с символа начала последовательности ```bos```, а с символа языка и символа начала последовательности ```rus bos```. \n",
    "\n",
    "Привидите несколько примеров удачно сгенерированных фамилий. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "X_names = ['bos ' + ' '.join(name) for name in X]\n",
    "Y_names = [' '.join(name) + ' eos' for name in X]\n",
    "maxlen = max([len(name) for name in X])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=len(alphabet)+2)\n",
    "tokenizer.fit_on_texts(X_names+Y_names)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(X_names)\n",
    "X_train = pad_sequences(sequences, maxlen=maxlen, padding='post')\n",
    "\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(Y_names)\n",
    "Y_train = pad_sequences(sequences, maxlen=maxlen,padding='post')\n",
    "temp = Y_train.copy()\n",
    "\n",
    "Y_train_cat  = [to_categorical(sent, num_classes=len(alphabet)+2) for sent in Y_train]\n",
    "Y_train =  np.asarray(Y_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_index = tokenizer.word_index\n",
    "index_char = {i: c for c, i in char_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(len(alphabet)+2, 30, input_length=maxlen))\n",
    "model.add(LSTM(128, return_sequences = True))\n",
    "\n",
    "model.add(Dense(len(alphabet)+2, activation = 'softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(1, 20):\n",
    "    X_train_shuffled, y_train_shuffled = shuffle(X_train, Y_train)\n",
    "    model.fit(X_train_shuffled, y_train_shuffled, batch_size=len(X_train), epochs=1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) #/ temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.choice(range(len(alphabet)+2), p = preds)\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = ''\n",
    "seed = 'bos'\n",
    "generated += seed + ' '\n",
    "print('----- Generating with seed: \"' + seed + '\"')\n",
    "print(generated)\n",
    "\n",
    "\n",
    "for i in range(7): \n",
    "    sequences = tokenizer.texts_to_sequences([seed])\n",
    "    X_pred = pad_sequences(sequences, maxlen=maxlen, padding = 'post')\n",
    "\n",
    "    preds = model.predict(X_pred, verbose=0)[0]\n",
    "    samples = [sample(p) for p in preds]\n",
    "    next_index = samples[i]\n",
    "    while next_index == 0 or next_index == 10:\n",
    "        samples = [sample(p) for p in preds]\n",
    "        next_index = samples[i]\n",
    "    if next_index > 55:\n",
    "        next_index = 55\n",
    "    next_char = index_char[next_index]\n",
    "    generated += next_char + ' '\n",
    "    print(generated)\n",
    "    seed += next_char\n",
    "    if next_char == 'eos':\n",
    "        break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
